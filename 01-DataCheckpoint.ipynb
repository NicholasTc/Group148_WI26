{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Rubric\n",
    "\n",
    "Instructions: DELETE this cell before you submit via a `git push` to your repo before deadline. This cell is for your reference only and is not needed in your report. \n",
    "\n",
    "Scoring: Out of 10 points\n",
    "\n",
    "- Each Developing  => -2 pts\n",
    "- Each Unsatisfactory/Missing => -4 pts\n",
    "  - until the score is \n",
    "\n",
    "If students address the detailed feedback in a future checkpoint they will earn these points back\n",
    "\n",
    "\n",
    "|                  | Unsatisfactory                                                                                                                                                                                                    | Developing                                                                                                                                                                                              | Proficient                                     | Excellent                                                                                                                              |\n",
    "|------------------|-------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|------------------------------------------------|----------------------------------------------------------------------------------------------------------------------------------------|\n",
    "| Data relevance   | Did not have data relevant to their question. Or the datasets don't work together because there is no way to line them up against each other. If there are multiple datasets, most of them have this trouble | Data was only tangentially relevant to the question or a bad proxy for the question. If there are multiple datasets, some of them may be irrelevant or can't be easily combined.                       | All data sources are relevant to the question. | Multiple data sources for each aspect of the project. It's clear how the data supports the needs of the project.                         |\n",
    "| Data description | Dataset or its cleaning procedures are not described. If there are multiple datasets, most have this trouble                                                                                              | Data was not fully described. If there are multiple datasets, some of them are not fully described                                                                                                      | Data was fully described                       | The details of the data descriptions and perhaps some very basic EDA also make it clear how the data supports the needs of the project. |\n",
    "| Data wrangling   | Did not obtain data. They did not clean/tidy the data they obtained.  If there are multiple datasets, most have this trouble                                                                                 | Data was partially cleaned or tidied. Perhaps you struggled to verify that the data was clean because they did not present it well. If there are multiple datasets, some have this trouble | The data is cleaned and tidied.                | The data is spotless and they used tools to visualize the data cleanliness and you were convinced at first glance                      |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# COGS 108 - Data Checkpoint"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Authors\n",
    "\n",
    "Instructions: REPLACE the contents of this cell with your team list and their contributions. Note that this will change over the course of the checkpoints\n",
    "\n",
    "This is a modified [CRediT taxonomy of contributions](https://credit.niso.org). For each group member please list how they contributed to this project using these terms:\n",
    "> Analysis, Background research, Conceptualization, Data curation, Experimental investigation, Methodology, Project administration, Software, Visualization, Writing – original draft, Writing – review & editing\n",
    "\n",
    "Example team list and credits:\n",
    "- Alice Anderson: Conceptualization, Data curation, Methodology, Writing - original draft\n",
    "- Bob Barker:  Analysis, Software, Visualization\n",
    "- Charlie Chang: Project administration, Software, Writing - review & editing\n",
    "- Dani Delgado: Analysis, Background research, Visualization, Writing - original draft"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Research Question"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Instructions: REPLACE the contents of this cell with your work, including any updates to recover points lost in your proposal feedback\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Background and Prior Work"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Instructions: REPLACE the contents of this cell with your work, including any updates to recover points lost in your proposal feedback"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hypothesis\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Instructions: REPLACE the contents of this cell with your work, including any updates to recover points lost in your proposal feedback\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data overview\n",
    "\n",
    "Instructions: REPLACE the contents of this cell with descriptions of your actual datasets.\n",
    "\n",
    "For each dataset include the following information\n",
    "- Dataset #1\n",
    "  - Dataset Name:\n",
    "  - Link to the dataset:\n",
    "  - Number of observations:\n",
    "  - Number of variables:\n",
    "  - Description of the variables most relevant to this project\n",
    "  - Descriptions of any shortcomings this dataset has with repsect to the project\n",
    "- Dataset #2 (if you have more than one!)\n",
    "  - same as above\n",
    "- etc\n",
    "\n",
    "Each dataset deserves either a set of bullet points as above or a few sentences if you prefer that method.\n",
    "\n",
    "If you plan to use multiple datasets, add a few sentences about how you plan to combine these datasets.\n",
    "\n",
    "- Dataset #2\n",
    "  - Dataset Name: **Mental Health & Social Media Balance Dataset**\n",
    "  - Link to the dataset: `https://www.kaggle.com/datasets/prince7489/mental-health-and-social-media-balance-dataset?`\n",
    "  - Number of observations: 500\n",
    "  - Number of variables: 10\n",
    "  - Description of the variables most relevant to this project:\n",
    "    - Daily Screen Time (Hours): Average hours per day spent on social media. This is our primary variable of interest.\n",
    "    - Sleep Quality (1–10 scale): Self reported sleep quality, where higher scores indicate better perceived sleep.\n",
    "    - Stress Level (1–10 scale): Self reported stress level, where higher values indicate higher stress levels.\n",
    "    - Happiness Index (1–10 scale): Self reported happiness, where higher scores indicate greater subjective well being.\n",
    "  - Descriptions of any shortcomings this dataset has with respect to the project:\n",
    "    - Lacks data to distinguish whether social media use was passive consumption or active interaction.\n",
    "    - Most data is self reported so there may be biases or inaccurately reported data.\n",
    "- Dataset #3  \n",
    "  - Dataset Name: **Social Media Usage and Emotional Well-Being**  \n",
    "  - Link to the dataset: `https://www.kaggle.com/datasets/emirhanai/social-media-usage-and-emotional-well-being`  \n",
    "  - Number of observations: Uses 3 Observations:  \n",
    "    - train.csv: 1001 rows  \n",
    "    - val.csv: 145 rows  \n",
    "    - test.csv: 103 rows  \n",
    "  - Number of variables: **10** columns  \n",
    "\n",
    "  - Description of the variables most relevant to this project:  \n",
    "    This dataset is basically “social media behavior metrics” + a label for emotion. The main columns are:  \n",
    "\n",
    "    - User_ID: unique user identifier  \n",
    "    - Age (years) and Gender: basic demographics  \n",
    "    - Platform: which social media platform the user is associated with (ex: Instagram, Twitter, Facebook, etc.)  \n",
    "    - Daily_Usage_Time (minutes): time spent per day on the platform, measured in minutes  \n",
    "    - Posts_Per_Day / Likes_Received_Per_Day / Comments_Received_Per_Day / Messages_Sent_Per_Day: daily activity/engagement counts (all are “counts per day”)  \n",
    "    - Dominant_Emotion: the main emotion label for that user/day (ex: Happiness, Sadness, Anger, Anxiety, Boredom, Neutral)  \n",
    "\n",
    "  - Shortcomings / limitations:  \n",
    "    - Small dataset size (especially val/test). That limits how confident we can be about patterns/generalization.  \n",
    "    - Some community analyses report messy rows / parsing issues (ex: a “User_ID” that turns into text) and potential duplicates / inconsistencies in splits, so we should check types, bad rows, duplicates, and whether IDs overlap across files.  \n",
    "    - This is behavior data + a label, but we don’t have context like why the person felt that way, mental health history, or real clinical measures. So “emotion” here is just what the dataset defines, not a diagnosis.  \n",
    "\n",
    "- Dataset #4\n",
    "  - Dataset Name: **Social Media and Mental Health** (survey; `smmh.csv`)\n",
    "  - Link to the dataset (official Kaggle page): `https://www.kaggle.com/datasets/souvikahmed071/social-media-and-mental-health`\n",
    "  - Reproducible raw CSV mirror used in this repo (no Kaggle login required): `https://raw.githubusercontent.com/Daerkns/correlation-between-social-media-and-mental-health/main/smmh.csv`\n",
    "  - Number of observations: **481** survey responses (rows)\n",
    "  - Number of variables: **21** columns\n",
    "  - Description of the variables most relevant to this project:\n",
    "    - Demographics/controls: age, gender, relationship status, occupation status, affiliated organization type\n",
    "    - Social media usage: whether they use social media, which platforms (comma-separated list), and average time spent per day (categorical bins)\n",
    "    - Wellbeing / mental health proxies (mostly Likert 1–5): distractedness, worries, concentration difficulty, social comparison frequency + feelings about comparisons, validation seeking, feeling depressed/down, interest fluctuation, sleep issues\n",
    "  - Shortcomings / limitations:\n",
    "    - Cross-sectional and self-reported survey data (no causal conclusions; subject to recall/social desirability bias)\n",
    "    - Does not include direct “active interaction” counts (posting/commenting/DMs); we will treat it as a **supporting** dataset and use available proxies (time-spent, platforms, comparison/validation items)\n",
    "    - Potential sample bias (survey respondents are not representative of all adults)\n",
    "    - Small number of potentially inconsistent responses (e.g., `Do you use social media? = No` but other usage questions answered) that we will flag and handle explicitly in cleaning\n",
    "- Dataset #5\n",
    "  - Dataset Name: **Screen Time vs Mental Wellness Survey (2025)** (survey; `ScreenTime vs MentalWellness.csv`)\n",
    "  - Link to the dataset (official Kaggle page): `https://www.kaggle.com/datasets/adharshinikumar/screentime-vs-mentalwellness-survey-2025`\n",
    "  - Number of observations: **400**\n",
    "  - Number of variables: **15**\n",
    "  - Description of the variables most relevant to this project:\n",
    "    - Leisure Screen Time (Hours): Total time spent on social media, gaming, streaming. This serves as a proxy for passive digital exposure.\n",
    "    - Sleep Hours: Total sleep duration.\n",
    "    - Sleep Quality (1-5 scale): Self-reported sleep-related measure indicating rest quality.\n",
    "    - Stress Level (0-10 scale): Self-reported stress level.\n",
    "    - Mental Wellness Indicators: Composite or self-rated measure of overall mental wellbeing indicating mood, energy, focus.\n",
    "  - Descriptions of any shortcomings this dataset has with respect to the project:\n",
    "    - Not social media specific making it impossible to isolate the effects of social media alone\n",
    "    - Does not distinguish between passive consumption and active interaction.\n",
    "    - Most variables are self-reported, introducing potential reporting bias.\n",
    "    - Composite wellness indicator aggregates multiple dimensions, which may mask specific pathways between screen time and individual outcomes\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run this code every time when you're actively developing modules in .py files.  It's not needed if you aren't making modules\n",
    "#\n",
    "## this code is necessary for making sure that any modules we load are updated here \n",
    "## when their source code .py files are modified\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Already exists: data/00-raw/Mental_Health_and_Social_Media_Balance_Dataset.csv\n",
      "Already exists: data/00-raw/smmh.csv\n"
     ]
    }
   ],
   "source": [
    "# Setup code -- this only needs to be run once after cloning the repo!\n",
    "# This cell downloads raw data into `data/00-raw/`.\n",
    "\n",
    "# If you're missing packages in your environment, uncomment this line\n",
    "# %pip install pandas numpy matplotlib seaborn\n",
    "\n",
    "import os\n",
    "import urllib.request\n",
    "\n",
    "# Ensure expected data directories exist.\n",
    "os.makedirs('data/00-raw', exist_ok=True)\n",
    "os.makedirs('data/01-interim', exist_ok=True)\n",
    "os.makedirs('data/02-processed', exist_ok=True)\n",
    "\n",
    "# Dataset #2\n",
    "dataset_2_URL = 'https://raw.githubusercontent.com/NeenosY/Group148_WI26/refs/heads/master/data/00-raw/Mental_Health_and_Social_Media_Balance_Dataset.csv'\n",
    "dataset_2_LOCAL = 'data/00-raw/Mental_Health_and_Social_Media_Balance_Dataset.csv'\n",
    "\n",
    "# Dataset #3 (Social Media Usage and Emotional Well-Being)\n",
    "EMIR_DIR = 'data/00-raw/emirhanai'\n",
    "EMIR_FILES = ['train.csv', 'val.csv', 'test.csv']\n",
    "\n",
    "# Dataset #4 (Social Media and Mental Health)\n",
    "SMMH_URL = 'https://raw.githubusercontent.com/Daerkns/correlation-between-social-media-and-mental-health/main/smmh.csv'\n",
    "SMMH_LOCAL = 'data/00-raw/smmh.csv'\n",
    "\n",
    "def download_if_missing(url: str, local_path: str) -> None:\n",
    "    if os.path.exists(local_path) and os.path.getsize(local_path) > 0:\n",
    "        print(f\"Already exists: {local_path}\")\n",
    "        return\n",
    "\n",
    "    print(f\"Downloading to: {local_path}\")\n",
    "    urllib.request.urlretrieve(url, local_path)\n",
    "    print(f\"Downloaded: {local_path} ({os.path.getsize(local_path)} bytes)\")\n",
    "\n",
    "download_if_missing(dataset_2_URL, dataset_2_LOCAL)\n",
    "download_if_missing(SMMH_URL, SMMH_LOCAL)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataset #1 \n",
    "\n",
    "Instructions: \n",
    "1. Change the header from Dataset #1 to something more descriptive of the dataset\n",
    "2. Write a few paragraphs about this dataset. Make sure to cover\n",
    "   1. Describe the important metrics, what units they are in, and giv some sense of what they mean.  For example \"Fasting blood glucose in units of mg glucose per deciliter of blood.  Normal values for healthy individuals range from 70 to 100 mg/dL.  Values 100-125 are prediabetic and values >125mg/dL indicate diabetes. Values <70 indicate hypoglycemia. Fasting idicates the patient hasn't eaten in the last 8 hours.  If blood glucose is >250 or <50 at any time (regardless of the time of last meal) the patient's life may be in immediate danger\"\n",
    "   2. If there are any major concerns with the dataset, describe them. For example \"Dataset is composed of people who are serious enough about eating healthy that they voluntarily downloaded an app dedicated to tracking their eating patterns. This sample is likely biased because of that self-selection. These people own smartphones and may be healthier and may have more disposable income than the average person.  Those who voluntarily log conscientiously and for long amounts of time are also likely even more interested in health than those who download the app and only log a bit before getting tired of it\"\n",
    "3. Use the cell below to \n",
    "    1. load the dataset \n",
    "    2. make the dataset tidy or demonstrate that it was already tidy\n",
    "    3. demonstrate the size of the dataset\n",
    "    4. find out how much data is missing, where its missing, and if its missing at random or seems to have any systematic relationships in its missingness\n",
    "    5. find and flag any outliers or suspicious entries\n",
    "    6. clean the data or demonstrate that it was already clean.  You may choose how to deal with missingness (dropna of fillna... how='any' or 'all') and you should justify your choice in some way\n",
    "    7. You will load raw data from `data/00-raw/`, you will (optionally) write intermediate stages of your work to `data/01-interim` and you will write the final fully wrangled version of your data to `data/02-processed`\n",
    "4. Optionally you can also show some summary statistics for variables that you think are important to the project\n",
    "5. Feel free to add more cells here if that's helpful for you\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "Expected file 'instagram_usage_lifestyle.csv' not found in data/00-raw/. Current CSV files: ['Mental_Health_and_Social_Media_Balance_Dataset.csv', 'ScreenTime vs MentalWellness.csv', 'smmh.csv', 'test.csv', 'train.csv', 'val.csv']",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[3], line 18\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m csv_path\u001b[38;5;241m.\u001b[39mexists():\n\u001b[1;32m     17\u001b[0m     all_names \u001b[38;5;241m=\u001b[39m [p\u001b[38;5;241m.\u001b[39mname \u001b[38;5;28;01mfor\u001b[39;00m p \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28msorted\u001b[39m(RAW_DIR\u001b[38;5;241m.\u001b[39mglob(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m*.csv\u001b[39m\u001b[38;5;124m\"\u001b[39m))]\n\u001b[0;32m---> 18\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mFileNotFoundError\u001b[39;00m(\n\u001b[1;32m     19\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mExpected file \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mcsv_path\u001b[38;5;241m.\u001b[39mname\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m not found in data/00-raw/. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     20\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCurrent CSV files: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mall_names\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     21\u001b[0m     )\n\u001b[1;32m     23\u001b[0m df_raw \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mread_csv(csv_path)\n\u001b[1;32m     25\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUsing file: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mcsv_path\u001b[38;5;241m.\u001b[39mname\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: Expected file 'instagram_usage_lifestyle.csv' not found in data/00-raw/. Current CSV files: ['Mental_Health_and_Social_Media_Balance_Dataset.csv', 'ScreenTime vs MentalWellness.csv', 'smmh.csv', 'test.csv', 'train.csv', 'val.csv']"
     ]
    }
   ],
   "source": [
    "## Dataset #1 wrangling (Social Media User Activity Dataset)\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "\n",
    "RAW_DIR = Path(\"data/00-raw\")\n",
    "INTERIM_DIR = Path(\"data/01-interim\")\n",
    "PROCESSED_DIR = Path(\"data/02-processed\")\n",
    "\n",
    "RAW_DIR.mkdir(parents=True, exist_ok=True)\n",
    "INTERIM_DIR.mkdir(parents=True, exist_ok=True)\n",
    "PROCESSED_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "csv_path = RAW_DIR / \"instagram_usage_lifestyle.csv\"\n",
    "if not csv_path.exists():\n",
    "    all_names = [p.name for p in sorted(RAW_DIR.glob(\"*.csv\"))]\n",
    "    raise FileNotFoundError(\n",
    "        f\"Expected file '{csv_path.name}' not found in data/00-raw/. \"\n",
    "        f\"Current CSV files: {all_names}\"\n",
    "    )\n",
    "\n",
    "df_raw = pd.read_csv(csv_path)\n",
    "\n",
    "print(f\"Using file: {csv_path.name}\")\n",
    "print(f\"Shape: {df_raw.shape}\")\n",
    "print(\"\\nColumns:\")\n",
    "for c in df_raw.columns:\n",
    "    print(\"-\", c)\n",
    "\n",
    "display(df_raw.head())\n",
    "\n",
    "# ---------- Data quality checks ----------\n",
    "df = df_raw.copy()\n",
    "\n",
    "print(\"\\nRows, columns:\", df.shape)\n",
    "print(\"Duplicate rows:\", df.duplicated().sum())\n",
    "\n",
    "missing = (\n",
    "    df.isna()\n",
    "    .sum()\n",
    "    .to_frame(\"missing_count\")\n",
    "    .assign(missing_pct=lambda x: (x[\"missing_count\"] / len(df) * 100).round(2))\n",
    "    .sort_values(\"missing_count\", ascending=False)\n",
    ")\n",
    "print(\"\\nTop missingness columns:\")\n",
    "display(missing.head(25))\n",
    "\n",
    "print(\"\\nData types:\")\n",
    "display(df.dtypes.to_frame(\"dtype\"))\n",
    "\n",
    "def suggest_columns(columns, keywords, regex=False):\n",
    "    out = []\n",
    "    for c in columns:\n",
    "        cl = str(c).lower()\n",
    "        if regex:\n",
    "            if any(re.search(k, cl) for k in keywords):\n",
    "                out.append(c)\n",
    "        else:\n",
    "            if any(k in cl for k in keywords):\n",
    "                out.append(c)\n",
    "    return out\n",
    "\n",
    "candidate_map = {\n",
    "    \"age\": suggest_columns(df.columns, [r\"(^|[_\\s\\-])age([_\\s\\-]|$)\"], regex=True),\n",
    "    \"passive_use\": suggest_columns(df.columns, [\"passive\", \"scroll\", \"browse\", \"view\", \"watch\", \"consume\", \"time\", \"minutes\"]),\n",
    "    \"active_use\": suggest_columns(df.columns, [\"active\", \"post\", \"comment\", \"like\", \"message\", \"dm\", \"interact\"]),\n",
    "    \"stress\": suggest_columns(df.columns, [\"stress\", \"anxiety\", \"anx\"]),\n",
    "    \"happiness\": suggest_columns(df.columns, [\"happiness\", \"happy\", \"wellbeing\", \"well-being\", \"mood\"]),\n",
    "}\n",
    "\n",
    "print(\"\\nCandidate columns by concept:\")\n",
    "for k, v in candidate_map.items():\n",
    "    print(f\"\\n{k}:\")\n",
    "    if v:\n",
    "        for name in v:\n",
    "            print(\" -\", name)\n",
    "    else:\n",
    "        print(\" - (no obvious match found)\")\n",
    "\n",
    "# ---------- Cleaning config ----------\n",
    "col_map = {\n",
    "    \"age\": \"age\",\n",
    "    \"passive_use\": \"time_on_feed_per_day\",\n",
    "    \"active_use\": \"posts_created_per_week\",\n",
    "    \"stress\": \"perceived_stress_score\",\n",
    "    \"happiness\": \"self_reported_happiness\",\n",
    "}\n",
    "\n",
    "control_cols = [\n",
    "    \"gender\",\n",
    "    \"country\",\n",
    "    \"urban_rural\",\n",
    "    \"income_level\",\n",
    "    \"employment_status\",\n",
    "    \"education_level\",\n",
    "    \"exercise_hours_per_week\",\n",
    "    \"sleep_hours_per_night\",\n",
    "    \"weekly_work_hours\",\n",
    "]\n",
    "\n",
    "\n",
    "def to_snake(s):\n",
    "    s = str(s).strip().lower()\n",
    "    s = re.sub(r\"[^a-z0-9]+\", \"_\", s)\n",
    "    s = re.sub(r\"_+\", \"_\", s).strip(\"_\")\n",
    "    return s\n",
    "\n",
    "\n",
    "df_clean = df.copy()\n",
    "df_clean = df_clean.rename(columns={c: to_snake(c) for c in df_clean.columns})\n",
    "\n",
    "mapped = {k: (to_snake(v) if v is not None else None) for k, v in col_map.items()}\n",
    "control_cols_snake = [to_snake(c) for c in control_cols]\n",
    "\n",
    "selected_cols = [v for v in mapped.values() if v is not None] + control_cols_snake\n",
    "selected_cols = [c for c in selected_cols if c in df_clean.columns]\n",
    "\n",
    "if selected_cols:\n",
    "    df_clean = df_clean[selected_cols].copy()\n",
    "else:\n",
    "    print(\"\\nNo col_map/control_cols selected yet. Keeping all columns for now.\")\n",
    "\n",
    "for key in [\"age\", \"passive_use\", \"active_use\", \"stress\", \"happiness\"]:\n",
    "    col = mapped.get(key)\n",
    "    if col is not None and col in df_clean.columns:\n",
    "        df_clean[col] = pd.to_numeric(df_clean[col], errors=\"coerce\")\n",
    "\n",
    "if mapped[\"age\"] is not None and mapped[\"age\"] in df_clean.columns:\n",
    "    before = len(df_clean)\n",
    "    df_clean = df_clean[df_clean[mapped[\"age\"]] >= 18]\n",
    "    print(f\"Age filter (18+): {before} -> {len(df_clean)} rows\")\n",
    "else:\n",
    "    print(\"Age column not mapped; 18+ filter skipped.\")\n",
    "\n",
    "before_dup = len(df_clean)\n",
    "df_clean = df_clean.drop_duplicates()\n",
    "print(f\"Drop duplicates: {before_dup} -> {len(df_clean)} rows\")\n",
    "\n",
    "required_cols = [\n",
    "    mapped[k]\n",
    "    for k in [\"passive_use\", \"active_use\", \"stress\", \"happiness\"]\n",
    "    if mapped[k] is not None and mapped[k] in df_clean.columns\n",
    "]\n",
    "if required_cols:\n",
    "    before_req = len(df_clean)\n",
    "    df_clean = df_clean.dropna(subset=required_cols)\n",
    "    print(f\"Drop NA in required vars: {before_req} -> {len(df_clean)} rows\")\n",
    "else:\n",
    "    print(\"Required vars not fully mapped yet; NA filtering skipped.\")\n",
    "\n",
    "numeric_cols = df_clean.select_dtypes(include=[np.number]).columns.tolist()\n",
    "outlier_counts = {}\n",
    "for c in numeric_cols:\n",
    "    s = df_clean[c].dropna()\n",
    "    if len(s) < 5:\n",
    "        outlier_counts[c] = 0\n",
    "        continue\n",
    "    q1, q3 = s.quantile(0.25), s.quantile(0.75)\n",
    "    iqr = q3 - q1\n",
    "    if iqr == 0:\n",
    "        outlier_counts[c] = 0\n",
    "        continue\n",
    "    lo, hi = q1 - 1.5 * iqr, q3 + 1.5 * iqr\n",
    "    outlier_counts[c] = int(((df_clean[c] < lo) | (df_clean[c] > hi)).sum())\n",
    "\n",
    "outlier_report = pd.Series(outlier_counts, name=\"outlier_count\").sort_values(ascending=False).to_frame()\n",
    "print(\"\\nOutlier counts (IQR):\")\n",
    "display(outlier_report.head(20))\n",
    "\n",
    "interim_path = INTERIM_DIR / \"social_media_user_activity_interim.csv\"\n",
    "processed_path = PROCESSED_DIR / \"social_media_user_activity_cleaned.csv\"\n",
    "\n",
    "df_clean.to_csv(interim_path, index=False)\n",
    "df_clean.to_csv(processed_path, index=False)\n",
    "\n",
    "print(\"\\nFinal cleaned shape:\", df_clean.shape)\n",
    "print(\"Saved interim:\", interim_path)\n",
    "print(\"Saved processed:\", processed_path)\n",
    "display(df_clean.head())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataset #2: Mental Health and Social Media Balance Dataset\n",
    "\n",
    "The Mental Health and Social Media Balance Dataset comes from self reported surveys where people shared info about how much time they spend on social media and how they’re doing mentally. It includes stuff like average daily hours on apps, self rated stress levels, the quality of their sleep, a happiness score, and some basic demographics like age and gender too. The whole point of the dataset is to look at whether there’s any connection between heavy social media use and things like stress, sleep problems, or just feeling less happy overall. An important thing to not is that everything was collected at basically the same moment in time. So we can spot patterns and associations (like “people who report 4+ hours of scrolling tend to also report higher stress”), but we can’t say for sure that one thing is causing the other. This dataset will mainly be used to enforce our other datasets as this one can only be used to showcase consumption of social medias effect on wellbeing as a whole (not passive consumption versus active interaction). It lacks the data of whether or not the reported person is active or not on the platforms they spent time on.\n",
    "\n",
    "### Important metrics in the dataset:\n",
    "- Daily Screen Time (Hours)\n",
    "- Sleep Quality (1-10)\n",
    "  - 1 Being the worst sleep (No reported 1's)\n",
    "  - 10 Being the best sleep\n",
    "- Stress Level (1-10)\n",
    "  - 1 Being no stress (No reported 1's)\n",
    "  - 10 Being the highest level of stress\n",
    "- Happiness Index (1-10)\n",
    "  - 1 Being the least happy (No reports lower than 4)\n",
    "  - 10 Being the most happy\n",
    "These are the metrics that are directly tied to our research question on the effect of social media use (Screen Time (Hours)) on an adults mental wellbeing. Again, although this does not directly answer our question of the effects of the different kinds of social media use (passive consumption versus active interaction), it contains critical data to conclude whether or not social media even has an effect on an Adults mental wellbeing in the first place.\n",
    "\n",
    "### Potential concerns and biases:\n",
    "- Age (16-49) // Potential bias\n",
    "  - There are users under the age of 18 who reported to the dataset. These must be cleaned out to fit our research questions demographics (18+).\n",
    "  - There is a large range of ages (18-49 after cleaning), in which age biases might need to be taken into account. Things such, \"as are older people more happy in general?\", \"Do younger people use social media more often than older people?\", etc.\n",
    "- Gender (Male, Female, Other) // Potential bias\n",
    "  - Similarly to the differences in age, there might be biases of happiness, stress, screen time, and sleep quality when it comes to differences in gender (much higher or lower overalls despite screen time). These need to be taken into account and checked before coming to a conclusion with the compiled data.\n",
    "- Exercise Frequency (0-7 days a week) // Potential bias\n",
    "  - Need to check the overall comparison of whether higher or lower excerise frequency directly relate to happiness or stress levels to conclude whether it may or may not be a bias.\n",
    "- Self Report bias/concern // Potential bias and/or concern\n",
    "  - Since the data was mostly, if not all, self reported, there may be some innaccuracy in the data. This can come in the form of underreporting social media use or overinflating happiness/stress levels."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "dataset_2 = pd.read_csv(\"data/00-raw/Mental_Health_and_Social_Media_Balance_Dataset.csv\")\n",
    "dataset_2.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data already looks relatively clean and all column values have relevancy towards our research topic.\n",
    "\n",
    "We must first check whether there are any missing values in the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(dataset_2.isnull().any())\n",
    "print(\"Number of missing values: \" + str(dataset_2.isnull().sum().sum()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are no missing values that need to be dealt with.\n",
    "\n",
    "We are only interested in the responses of those 18 and older so we must filter out the extra responses."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_2 = dataset_2[dataset_2['Age'] >= 18]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets now check impossible values (ones that exceed the 1-10 limits or 7 day week limit)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_2.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "All max and min values are valid in their respective ranges.\n",
    "\n",
    "Finally, lets clean up the column names and values to be more visually appropriate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "columnNames = ['ID', 'Age', 'Gender', 'Screen Time', 'Sleep Quality', 'Stress Level', 'Days Without Social Media', 'Exercise Frequency', 'Platform', 'Happiness Level']\n",
    "dataset_2.columns = columnNames\n",
    "columnValues = ['Sleep Quality', 'Stress Level', 'Days Without Social Media', 'Exercise Frequency', 'Happiness Level']\n",
    "dataset_2 = dataset_2.drop(columns=['ID'])\n",
    "dataset_2[columnValues] = dataset_2[columnValues].astype(int)\n",
    "dataset_2 = dataset_2.reset_index(drop=True)\n",
    "dataset_2.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check how many responses are left after cleaning the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_2.to_csv(\"data/02-processed/Mental_Health_and_Social_Media_Balance_Dataset_processed.csv\", index=False)\n",
    "dataset_2.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataset #3: Social Media Usage and Emotional Well-Being (emirhanai)\n",
    "\n",
    "- **Dataset Name:** Social Media Usage and Emotional Well-Being\n",
    "- **Link to dataset:** https://www.kaggle.com/datasets/emirhanai/social-media-usage-and-emotional-well-being\n",
    "- **Raw files used:** `train.csv`, `val.csv`, `test.csv` (stored in `data/00-raw/emirhanai/`)\n",
    "- **Number of observations:** printed in the code cells below (train/val/test and combined)\n",
    "- **Number of variables:** printed in the code cells below\n",
    "\n",
    "**What this dataset contains:**\n",
    "Each row represents a single user record with:\n",
    "- Demographics (e.g., Age in years, Gender)\n",
    "- Social media context (e.g., Platform)\n",
    "- Usage/activity metrics (e.g., daily usage time in minutes, posts/likes/comments/messages per day)\n",
    "- A label representing emotional state (e.g., Dominant_Emotion)\n",
    "\n",
    "**Variables most relevant to our project:**\n",
    "- Daily usage time (minutes) and daily activity counts (posts/likes/comments/messages)\n",
    "- Platform + demographics (Age, Gender) for context/controls\n",
    "- Emotion label (Dominant_Emotion) as the outcome\n",
    "\n",
    "**Shortcomings / concerns:**\n",
    "- Observational data: we can talk about associations, not causation.\n",
    "- Emotion label is not a clinical diagnosis.\n",
    "- CSV may contain messy rows (we handle this during loading and cleaning).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'data/00-raw/emirhanai/train.csv'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[4], line 25\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21msafe_read_csv\u001b[39m(path):\n\u001b[1;32m     19\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m pd\u001b[38;5;241m.\u001b[39mread_csv(\n\u001b[1;32m     20\u001b[0m         path,\n\u001b[1;32m     21\u001b[0m         engine\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpython\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m     22\u001b[0m         on_bad_lines\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mskip\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     23\u001b[0m     )\n\u001b[0;32m---> 25\u001b[0m train \u001b[38;5;241m=\u001b[39m \u001b[43msafe_read_csv\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain_path\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     26\u001b[0m val   \u001b[38;5;241m=\u001b[39m safe_read_csv(val_path)\n\u001b[1;32m     27\u001b[0m test  \u001b[38;5;241m=\u001b[39m safe_read_csv(test_path)\n",
      "Cell \u001b[0;32mIn[4], line 19\u001b[0m, in \u001b[0;36msafe_read_csv\u001b[0;34m(path)\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21msafe_read_csv\u001b[39m(path):\n\u001b[0;32m---> 19\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mpd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread_csv\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     20\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpath\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     21\u001b[0m \u001b[43m        \u001b[49m\u001b[43mengine\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mpython\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     22\u001b[0m \u001b[43m        \u001b[49m\u001b[43mon_bad_lines\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mskip\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\n\u001b[1;32m     23\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/pandas/io/parsers/readers.py:1026\u001b[0m, in \u001b[0;36mread_csv\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, date_format, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options, dtype_backend)\u001b[0m\n\u001b[1;32m   1013\u001b[0m kwds_defaults \u001b[38;5;241m=\u001b[39m _refine_defaults_read(\n\u001b[1;32m   1014\u001b[0m     dialect,\n\u001b[1;32m   1015\u001b[0m     delimiter,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1022\u001b[0m     dtype_backend\u001b[38;5;241m=\u001b[39mdtype_backend,\n\u001b[1;32m   1023\u001b[0m )\n\u001b[1;32m   1024\u001b[0m kwds\u001b[38;5;241m.\u001b[39mupdate(kwds_defaults)\n\u001b[0;32m-> 1026\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_read\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilepath_or_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/pandas/io/parsers/readers.py:620\u001b[0m, in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    617\u001b[0m _validate_names(kwds\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnames\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m))\n\u001b[1;32m    619\u001b[0m \u001b[38;5;66;03m# Create the parser.\u001b[39;00m\n\u001b[0;32m--> 620\u001b[0m parser \u001b[38;5;241m=\u001b[39m \u001b[43mTextFileReader\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilepath_or_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    622\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m chunksize \u001b[38;5;129;01mor\u001b[39;00m iterator:\n\u001b[1;32m    623\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m parser\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/pandas/io/parsers/readers.py:1620\u001b[0m, in \u001b[0;36mTextFileReader.__init__\u001b[0;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[1;32m   1617\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptions[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhas_index_names\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m kwds[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhas_index_names\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[1;32m   1619\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles: IOHandles \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m-> 1620\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_engine \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_make_engine\u001b[49m\u001b[43m(\u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mengine\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/pandas/io/parsers/readers.py:1880\u001b[0m, in \u001b[0;36mTextFileReader._make_engine\u001b[0;34m(self, f, engine)\u001b[0m\n\u001b[1;32m   1878\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m mode:\n\u001b[1;32m   1879\u001b[0m         mode \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m-> 1880\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles \u001b[38;5;241m=\u001b[39m \u001b[43mget_handle\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1881\u001b[0m \u001b[43m    \u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1882\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1883\u001b[0m \u001b[43m    \u001b[49m\u001b[43mencoding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mencoding\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1884\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcompression\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcompression\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1885\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmemory_map\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmemory_map\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1886\u001b[0m \u001b[43m    \u001b[49m\u001b[43mis_text\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mis_text\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1887\u001b[0m \u001b[43m    \u001b[49m\u001b[43merrors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mencoding_errors\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mstrict\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1888\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstorage_options\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mstorage_options\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1889\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1890\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1891\u001b[0m f \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles\u001b[38;5;241m.\u001b[39mhandle\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/pandas/io/common.py:873\u001b[0m, in \u001b[0;36mget_handle\u001b[0;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[1;32m    868\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(handle, \u001b[38;5;28mstr\u001b[39m):\n\u001b[1;32m    869\u001b[0m     \u001b[38;5;66;03m# Check whether the filename is to be opened in binary mode.\u001b[39;00m\n\u001b[1;32m    870\u001b[0m     \u001b[38;5;66;03m# Binary mode does not support 'encoding' and 'newline'.\u001b[39;00m\n\u001b[1;32m    871\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m ioargs\u001b[38;5;241m.\u001b[39mencoding \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m ioargs\u001b[38;5;241m.\u001b[39mmode:\n\u001b[1;32m    872\u001b[0m         \u001b[38;5;66;03m# Encoding\u001b[39;00m\n\u001b[0;32m--> 873\u001b[0m         handle \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\n\u001b[1;32m    874\u001b[0m \u001b[43m            \u001b[49m\u001b[43mhandle\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    875\u001b[0m \u001b[43m            \u001b[49m\u001b[43mioargs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    876\u001b[0m \u001b[43m            \u001b[49m\u001b[43mencoding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mioargs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mencoding\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    877\u001b[0m \u001b[43m            \u001b[49m\u001b[43merrors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43merrors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    878\u001b[0m \u001b[43m            \u001b[49m\u001b[43mnewline\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    879\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    880\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    881\u001b[0m         \u001b[38;5;66;03m# Binary mode\u001b[39;00m\n\u001b[1;32m    882\u001b[0m         handle \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mopen\u001b[39m(handle, ioargs\u001b[38;5;241m.\u001b[39mmode)\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'data/00-raw/emirhanai/train.csv'"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "\n",
    "raw_dir = Path(\"data/00-raw/emirhanai\")\n",
    "interim_dir = Path(\"data/01-interim\")\n",
    "processed_dir = Path(\"data/02-processed\")\n",
    "\n",
    "interim_dir.mkdir(parents=True, exist_ok=True)\n",
    "processed_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "train_path = raw_dir / \"train.csv\"\n",
    "val_path   = raw_dir / \"val.csv\"\n",
    "test_path  = raw_dir / \"test.csv\"\n",
    "\n",
    "train_path, val_path, test_path\n",
    "\n",
    "def safe_read_csv(path):\n",
    "    return pd.read_csv(\n",
    "        path,\n",
    "        engine=\"python\",\n",
    "        on_bad_lines=\"skip\"\n",
    "    )\n",
    "\n",
    "train = safe_read_csv(train_path)\n",
    "val   = safe_read_csv(val_path)\n",
    "test  = safe_read_csv(test_path)\n",
    "\n",
    "print(\"train:\", train.shape)\n",
    "print(\"val:  \", val.shape)\n",
    "print(\"test: \", test.shape)\n",
    "\n",
    "\n",
    "df = pd.concat([train, val, test], ignore_index=True)\n",
    "print(\"combined shape:\", df.shape)\n",
    "\n",
    "df.head()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Setup:** Imported the libraries we need and define the file paths for Dataset #3 (train/val/test). Also make sure the interim and processed folders exist so we can save cleaned data later."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Load data:** Read `train.csv`, `val.csv`, and `test.csv` from `data/00-raw/emirhanai/`. A few lines are malformed so to have the notebook run top-to-bottom without crashing we used a diff CSV loader."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Combine splits:** Concatenate train/val/test into one dataframe for cleaning and exploratory checks. This does not change the data values; it just makes it easier to analyze everything together."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make columns easier to work with (Tidy)\n",
    "df.columns = (\n",
    "    df.columns\n",
    "      .str.strip()\n",
    "      .str.replace(\" \", \"_\")\n",
    "      .str.replace(\"(\", \"\", regex=False)\n",
    "      .str.replace(\")\", \"\", regex=False)\n",
    ")\n",
    "\n",
    "print(df.columns.tolist())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Tidy check:** Confirm the dataset is already tidy (each row is one observation and each column is one variable). Also standardize column names to make them easier to reference in Python."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert Age to numeric (bad strings become NaN)\n",
    "if \"Age\" in df.columns:\n",
    "    df[\"Age\"] = pd.to_numeric(df[\"Age\"], errors=\"coerce\")\n",
    "\n",
    "# Same idea for usage + counts if they exist\n",
    "num_cols = [\n",
    "    \"Daily_Usage_Time_minutes\",\n",
    "    \"Posts_Per_Day\",\n",
    "    \"Likes_Received_Per_Day\",\n",
    "    \"Comments_Received_Per_Day\",\n",
    "    \"Messages_Sent_Per_Day\"\n",
    "]\n",
    "\n",
    "for c in num_cols:\n",
    "    if c in df.columns:\n",
    "        df[c] = pd.to_numeric(df[c], errors=\"coerce\")\n",
    "\n",
    "df.dtypes\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Data types:** Convert columns like Age and activity metrics to numeric types. Any non-numeric values become missing (`NaN`), which we handle later during cleaning.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Rows (observations):\", df.shape[0])\n",
    "print(\"Columns (variables):\", df.shape[1])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Dataset size:** Report the number of observations (rows) and variables (columns) so we can document dataset size for the checkpoint."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Missing Data\n",
    "missing_count = df.isna().sum().sort_values(ascending=False)\n",
    "missing_pct = (df.isna().mean() * 100).sort_values(ascending=False).round(2)\n",
    "\n",
    "missing_table = pd.DataFrame({\n",
    "    \"missing_count\": missing_count,\n",
    "    \"missing_pct\": missing_pct\n",
    "})\n",
    "\n",
    "display(missing_table[missing_table[\"missing_count\"] > 0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "**Missing data:** Calculate how much data is missing in each column and check where missingness happens. We also do a simple check to see if missing values are more common in some groups."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"flag_suspicious\"] = False\n",
    "\n",
    "# Age suspicious\n",
    "if \"Age\" in df.columns:\n",
    "    df.loc[(df[\"Age\"] < 10) | (df[\"Age\"] > 100), \"flag_suspicious\"] = True\n",
    "\n",
    "# Daily usage suspicious (minutes in a day is 0–1440)\n",
    "if \"Daily_Usage_Time_minutes\" in df.columns:\n",
    "    df.loc[(df[\"Daily_Usage_Time_minutes\"] < 0) | (df[\"Daily_Usage_Time_minutes\"] > 1440), \"flag_suspicious\"] = True\n",
    "\n",
    "# Negative counts suspicious\n",
    "count_cols = [\"Posts_Per_Day\",\"Likes_Received_Per_Day\",\"Comments_Received_Per_Day\",\"Messages_Sent_Per_Day\"]\n",
    "for c in count_cols:\n",
    "    if c in df.columns:\n",
    "        df.loc[df[c] < 0, \"flag_suspicious\"] = True\n",
    "\n",
    "print(\"Suspicious rows:\", int(df[\"flag_suspicious\"].sum()))\n",
    "display(df[df[\"flag_suspicious\"]].head(10))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Outliers & suspicious values:** Flag entries that are outside realistic ranges. We flag first so we can inspect before deciding to remove anything.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save interim version (after dtype fixes + suspicious flag)\n",
    "df.to_csv(interim_dir / \"emirhanai_interim.csv\", index=False)\n",
    "\n",
    "df_clean = df.copy()\n",
    "\n",
    "# 1) duplicates\n",
    "before = len(df_clean)\n",
    "df_clean = df_clean.drop_duplicates()\n",
    "print(\"Dropped duplicates:\", before - len(df_clean))\n",
    "\n",
    "# 2) suspicious\n",
    "before = len(df_clean)\n",
    "df_clean = df_clean[df_clean[\"flag_suspicious\"] == False].copy()\n",
    "print(\"Dropped suspicious:\", before - len(df_clean))\n",
    "\n",
    "# 3) missingness handling (drop rows missing key fields)\n",
    "key_cols = [\"Age\", \"Gender\", \"Platform\", \"Daily_Usage_Time_minutes\", \"Dominant_Emotion\"]\n",
    "key_cols = [c for c in key_cols if c in df_clean.columns]\n",
    "\n",
    "before = len(df_clean)\n",
    "df_clean = df_clean.dropna(subset=key_cols)\n",
    "print(\"Dropped missing key fields:\", before - len(df_clean))\n",
    "\n",
    "# Save processed\n",
    "df_clean.to_csv(processed_dir / \"emirhanai_processed.csv\", index=False)\n",
    "print(\"Final cleaned shape:\", df_clean.shape)\n",
    "df_clean.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_clean.describe().round()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Cleaning & saving:** Remove duplicates and clearly suspicious rows, then handle missing values in key columns. Save an interim dataset (`data/01-interim/`) and the final cleaned dataset (`data/02-processed/`) for later analysis/modeling."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Histogram (daily minutes)\n",
    "if \"Daily_Usage_Time_minutes\" in df_clean.columns:\n",
    "    plt.figure()\n",
    "    df_clean[\"Daily_Usage_Time_minutes\"].hist(bins=20)\n",
    "    plt.xlabel(\"Daily Usage Time (minutes)\")\n",
    "    plt.ylabel(\"Count\")\n",
    "    plt.show()\n",
    "\n",
    "# Bar chart (emotion)\n",
    "if \"Dominant_Emotion\" in df_clean.columns:\n",
    "    plt.figure()\n",
    "    df_clean[\"Dominant_Emotion\"].value_counts().plot(kind=\"bar\")\n",
    "    plt.xlabel(\"Dominant Emotion\")\n",
    "    plt.ylabel(\"Count\")\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Summary + visuals (optional):** Showed basic summary statistics and simple plots to better understand distributions (like daily usage time) and class balance (dominant emotion). This helps verify the dataset looks reasonable after cleaning."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataset #4: Social Media and Mental Health (`smmh.csv`)\n",
    "\n",
    "**Official dataset page (citation):** `https://www.kaggle.com/datasets/souvikahmed071/social-media-and-mental-health`\n",
    "\n",
    "**How we download it reproducibly for this repo:** Kaggle downloads require login/terms acceptance, so for reproducibility we download the same `smmh.csv` from a public raw mirror into `data/00-raw/` using the setup cell above.\n",
    "\n",
    "Each row is one survey response (timestamped). Columns include demographics, social media usage (platforms + time-spent category), and several mental-health / wellbeing proxy items (mostly Likert 1–5).\n",
    "\n",
    "In the wrangling below we:\n",
    "- Rename verbose survey-question column names to short `snake_case` names\n",
    "- Parse timestamps and coerce numeric Likert items\n",
    "- Standardize key categorical values (e.g., gender labels)\n",
    "- Engineer a numeric **hours/day** proxy from the time-spent categories\n",
    "- Expand the comma-separated platforms field into a small set of indicator columns\n",
    "- Check missingness, outliers, and a small set of internal inconsistencies\n",
    "- Save a processed dataset to `data/02-processed/smmh_processed.csv`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "raw_path = 'data/00-raw/smmh.csv'\n",
    "\n",
    "# Load raw data (downloaded in the setup cell)\n",
    "df_raw = pd.read_csv(raw_path, skipinitialspace=True)\n",
    "df_raw.columns = [c.strip() for c in df_raw.columns]\n",
    "\n",
    "rename_map = {\n",
    "    'Timestamp': 'timestamp',\n",
    "    '1. What is your age?': 'age',\n",
    "    '2. Gender': 'gender',\n",
    "    '3. Relationship Status': 'relationship_status',\n",
    "    '4. Occupation Status': 'occupation_status',\n",
    "    '5. What type of organizations are you affiliated with?': 'organization_affiliation',\n",
    "    '6. Do you use social media?': 'uses_social_media_answer',\n",
    "    '7. What social media platforms do you commonly use?': 'platforms',\n",
    "    '8. What is the average time you spend on social media every day?': 'avg_daily_time',\n",
    "    '9. How often do you find yourself using Social media without a specific purpose?': 'sm_no_purpose_freq',\n",
    "    '10. How often do you get distracted by Social media when you are busy doing something?': 'sm_distracted_freq',\n",
    "    \"11. Do you feel restless if you haven't used Social media in a while?\": 'sm_restless_freq',\n",
    "    '12. On a scale of 1 to 5, how easily distracted are you?': 'easily_distracted',\n",
    "    '13. On a scale of 1 to 5, how much are you bothered by worries?': 'bothered_by_worries',\n",
    "    '14. Do you find it difficult to concentrate on things?': 'difficulty_concentrating',\n",
    "    '15. On a scale of 1-5, how often do you compare yourself to other successful people through the use of social media?': 'compare_to_successful_freq',\n",
    "    '16. Following the previous question, how do you feel about these comparisons, generally speaking?': 'feel_about_comparisons',\n",
    "    '17. How often do you look to seek validation from features of social media?': 'seek_validation_freq',\n",
    "    '18. How often do you feel depressed or down?': 'depressed_or_down_freq',\n",
    "    '19. On a scale of 1 to 5, how frequently does your interest in daily activities fluctuate?': 'interest_fluctuation',\n",
    "    '20. On a scale of 1 to 5, how often do you face issues regarding sleep?': 'sleep_issues',\n",
    "}\n",
    "\n",
    "missing_cols = sorted(set(rename_map) - set(df_raw.columns))\n",
    "if missing_cols:\n",
    "    raise ValueError(f\"Unexpected raw columns (missing expected): {missing_cols}\")\n",
    "\n",
    "# Rename columns\n",
    "df = df_raw.rename(columns=rename_map).copy()\n",
    "\n",
    "# Parse timestamp + age\n",
    "df['timestamp'] = pd.to_datetime(df['timestamp'], errors='coerce')\n",
    "df['age'] = pd.to_numeric(df['age'], errors='coerce')\n",
    "\n",
    "# Standardize string columns\n",
    "string_cols = [\n",
    "    'gender',\n",
    "    'relationship_status',\n",
    "    'occupation_status',\n",
    "    'organization_affiliation',\n",
    "    'uses_social_media_answer',\n",
    "    'platforms',\n",
    "    'avg_daily_time',\n",
    "    'feel_about_comparisons',\n",
    "]\n",
    "for c in string_cols:\n",
    "    df[c] = df[c].astype('string').str.strip()\n",
    "\n",
    "# Normalize gender labels (keep it simple + transparent)\n",
    "def _normalize_gender(x):\n",
    "    if pd.isna(x):\n",
    "        return pd.NA\n",
    "    s = str(x).strip()\n",
    "    s_low = s.lower()\n",
    "\n",
    "    if s_low in {'nb', 'nonbinary', 'non-binary'}:\n",
    "        return 'Non-binary'\n",
    "    if s_low in {'unsure'}:\n",
    "        return 'Unsure'\n",
    "    if s_low in {'trans'}:\n",
    "        return 'Trans'\n",
    "\n",
    "    # Common values like 'Male'/'Female' stay as-is (titlecased)\n",
    "    return s.title()\n",
    "\n",
    "df['gender'] = df['gender'].map(_normalize_gender)\n",
    "\n",
    "# Coerce Likert-style items to numeric\n",
    "likert_cols = [\n",
    "    'sm_no_purpose_freq',\n",
    "    'sm_distracted_freq',\n",
    "    'sm_restless_freq',\n",
    "    'easily_distracted',\n",
    "    'bothered_by_worries',\n",
    "    'difficulty_concentrating',\n",
    "    'compare_to_successful_freq',\n",
    "    'seek_validation_freq',\n",
    "    'depressed_or_down_freq',\n",
    "    'interest_fluctuation',\n",
    "    'sleep_issues',\n",
    "]\n",
    "for c in likert_cols:\n",
    "    df[c] = pd.to_numeric(df[c], errors='coerce').astype('Int64')\n",
    "\n",
    "print('Raw shape:', df_raw.shape)\n",
    "print('Renamed/typed shape:', df.shape)\n",
    "df.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Feature engineering: platforms + time spent ---\n",
    "\n",
    "from IPython.display import display\n",
    "\n",
    "# Convert Yes/No to a boolean indicator (keep original + a cleaned version)\n",
    "df['uses_social_media_original'] = (\n",
    "    df['uses_social_media_answer']\n",
    "      .str.lower()\n",
    "      .map({'yes': True, 'no': False})\n",
    "      .astype('boolean')\n",
    ")\n",
    "\n",
    "common_platforms = [\n",
    "    'Facebook', 'Instagram', 'Twitter', 'YouTube',\n",
    "    'TikTok', 'Snapchat', 'Discord', 'Reddit', 'Pinterest'\n",
    "]\n",
    "_common_set = set(common_platforms)\n",
    "\n",
    "def _normalize_platform(p: str) -> str:\n",
    "    p = p.strip()\n",
    "    p_low = p.lower()\n",
    "\n",
    "    mapping = {\n",
    "        'youtube': 'YouTube',\n",
    "        'tik tok': 'TikTok',\n",
    "        'tiktok': 'TikTok',\n",
    "        'fb': 'Facebook',\n",
    "        'x': 'Twitter',\n",
    "    }\n",
    "    if p_low in mapping:\n",
    "        return mapping[p_low]\n",
    "\n",
    "    # Keep capitalization consistent for common platforms\n",
    "    for canon in common_platforms:\n",
    "        if p_low == canon.lower():\n",
    "            return canon\n",
    "\n",
    "    return p.title()\n",
    "\n",
    "def _parse_platforms(x):\n",
    "    if pd.isna(x):\n",
    "        return []\n",
    "    items = [i.strip() for i in str(x).split(',') if i.strip()]\n",
    "    norm = [_normalize_platform(i) for i in items]\n",
    "\n",
    "    # De-duplicate while preserving order\n",
    "    out = []\n",
    "    seen = set()\n",
    "    for p in norm:\n",
    "        if p not in seen:\n",
    "            out.append(p)\n",
    "            seen.add(p)\n",
    "    return out\n",
    "\n",
    "platform_list = df['platforms'].map(_parse_platforms)\n",
    "df['platform_count'] = platform_list.map(len)\n",
    "df['platforms_clean'] = platform_list.map(lambda lst: ', '.join(lst) if lst else pd.NA).astype('string')\n",
    "\n",
    "for p in common_platforms:\n",
    "    col = f\"uses_{p.lower()}\".replace(' ', '_')\n",
    "    df[col] = platform_list.map(lambda lst, p=p: p in lst).astype('boolean')\n",
    "\n",
    "df['uses_other_platform'] = platform_list.map(lambda lst: any(p not in _common_set for p in lst)).astype('boolean')\n",
    "\n",
    "# Map time-spent bins to an approximate numeric hours/day proxy.\n",
    "# Midpoints are used for closed intervals; \"More than 5 hours\" is set to 5.5 as a conservative constant.\n",
    "time_to_hours = {\n",
    "    'Less than an Hour': 0.5,\n",
    "    'Between 1 and 2 hours': 1.5,\n",
    "    'Between 2 and 3 hours': 2.5,\n",
    "    'Between 3 and 4 hours': 3.5,\n",
    "    'Between 4 and 5 hours': 4.5,\n",
    "    'More than 5 hours': 5.5,\n",
    "}\n",
    "df['avg_daily_hours'] = df['avg_daily_time'].map(time_to_hours).astype('Float64')\n",
    "\n",
    "# --- Data quality checks: missingness, outliers, inconsistencies ---\n",
    "\n",
    "print('\\nAge range (min/max):', df['age'].min(), df['age'].max())\n",
    "print('Age quantiles:')\n",
    "print(df['age'].quantile([0, 0.01, 0.25, 0.5, 0.75, 0.99, 1]).to_string())\n",
    "\n",
    "# Flag ages that are suspicious for an \"adults 18+\" research question\n",
    "suspicious_age_mask = (df['age'] < 18) | (df['age'] > 100)\n",
    "print('\\nSuspicious ages (<18 or >100):', int(suspicious_age_mask.sum()))\n",
    "if suspicious_age_mask.any():\n",
    "    display(df.loc[suspicious_age_mask, ['timestamp', 'age', 'gender']].head(10))\n",
    "\n",
    "# Missingness summary\n",
    "missing = df.isna().sum().sort_values(ascending=False)\n",
    "missing_pct = (df.isna().mean() * 100).round(2)\n",
    "missing_table = pd.DataFrame({'missing_count': missing, 'missing_pct': missing_pct}).query('missing_count > 0')\n",
    "print('\\nMissingness (only columns with missing values):')\n",
    "display(missing_table)\n",
    "\n",
    "# Inconsistency: answered \"No\" to using social media but still provided platforms or time spent\n",
    "inferred_use = (df['platform_count'] > 0) | df['avg_daily_hours'].notna()\n",
    "inconsistent = (df['uses_social_media_original'] == False) & inferred_use\n",
    "print('\\nInconsistent rows (uses_social_media_original == False but usage fields present):', int(inconsistent.sum()))\n",
    "if inconsistent.any():\n",
    "    display(df.loc[inconsistent, ['uses_social_media_answer', 'platforms', 'avg_daily_time', 'platform_count']].head(10))\n",
    "\n",
    "# Cleaning rule: keep the original answer, but also create a \"clean\" usage boolean that\n",
    "# treats any reported usage fields as evidence of actual use.\n",
    "df['uses_social_media'] = (\n",
    "    df['uses_social_media_original']\n",
    "      .fillna(False)\n",
    "      .astype('boolean')\n",
    "    | inferred_use.astype('boolean')\n",
    ")\n",
    "\n",
    "# Check Likert ranges\n",
    "likert_minmax = df[likert_cols].agg(['min', 'max']).T\n",
    "out_of_range = (df[likert_cols] < 1) | (df[likert_cols] > 5)\n",
    "print('\\nLikert min/max (should be within 1–5):')\n",
    "display(likert_minmax)\n",
    "print('Out-of-range Likert entries (count):', int(out_of_range.sum().sum()))\n",
    "\n",
    "# --- Save processed dataset ---\n",
    "\n",
    "processed_cols = [\n",
    "    'timestamp', 'age', 'gender', 'relationship_status', 'occupation_status', 'organization_affiliation',\n",
    "    'uses_social_media_original', 'uses_social_media',\n",
    "    'avg_daily_time', 'avg_daily_hours',\n",
    "    'platforms_clean', 'platform_count',\n",
    "] + [\n",
    "    f\"uses_{p.lower()}\".replace(' ', '_') for p in common_platforms\n",
    "] + [\n",
    "    'uses_other_platform',\n",
    "    'sm_no_purpose_freq', 'sm_distracted_freq', 'sm_restless_freq',\n",
    "    'easily_distracted', 'bothered_by_worries', 'difficulty_concentrating',\n",
    "    'compare_to_successful_freq', 'feel_about_comparisons', 'seek_validation_freq',\n",
    "    'depressed_or_down_freq', 'interest_fluctuation', 'sleep_issues'\n",
    "]\n",
    "\n",
    "processed_path = 'data/02-processed/smmh_processed.csv'\n",
    "df_processed = df[processed_cols].copy()\n",
    "df_processed.to_csv(processed_path, index=False)\n",
    "\n",
    "print(f\"\\nSaved processed dataset to: {processed_path}\")\n",
    "print('Processed shape:', df_processed.shape)\n",
    "df_processed.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Minimal EDA (sanity-check visuals) ---\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "sns.set_theme(style='whitegrid')\n",
    "\n",
    "# Distribution of time spent (numeric proxy)\n",
    "plt.figure(figsize=(7, 4))\n",
    "sns.histplot(df_processed['avg_daily_hours'].dropna().astype(float), bins=12)\n",
    "plt.title('Approx. Daily Social Media Use (hours/day)')\n",
    "plt.xlabel('avg_daily_hours')\n",
    "plt.ylabel('count')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Gender distribution\n",
    "plt.figure(figsize=(7, 4))\n",
    "order = df_processed['gender'].value_counts(dropna=False).index\n",
    "sns.countplot(data=df_processed, y='gender', order=order)\n",
    "plt.title('Gender distribution')\n",
    "plt.xlabel('count')\n",
    "plt.ylabel('gender')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Relationship between time spent and sleep issues (Likert 1–5)\n",
    "eda_tmp = df_processed.dropna(subset=['avg_daily_hours', 'sleep_issues']).copy()\n",
    "eda_tmp['avg_daily_hours'] = eda_tmp['avg_daily_hours'].astype(float)\n",
    "eda_tmp['sleep_issues'] = pd.to_numeric(eda_tmp['sleep_issues'], errors='coerce')\n",
    "\n",
    "plt.figure(figsize=(7, 4))\n",
    "sns.regplot(\n",
    "    data=eda_tmp,\n",
    "    x='avg_daily_hours',\n",
    "    y='sleep_issues',\n",
    "    x_jitter=0.05,\n",
    "    y_jitter=0.05,\n",
    "    scatter_kws={'alpha': 0.25, 's': 18},\n",
    "    line_kws={'color': 'black'}\n",
    ")\n",
    "plt.title('Time on Social Media vs Sleep Issues (proxy)')\n",
    "plt.xlabel('avg_daily_hours')\n",
    "plt.ylabel('sleep_issues (1–5)')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataset #5: Screen Time vs Mental Wellness Survey\n",
    "\n",
    "**Official dataset page:** `https://www.kaggle.com/datasets/adharshinikumar/screentime-vs-mentalwellness-survey-2025`\n",
    "\n",
    "**Download method:** Through Kaggle (login and terms acceptance required)\n",
    "\n",
    "**Description**\n",
    "The Screen Time vs Mental Wellness Survey dataset is based on self-reported survey responses examining how leisure screen time exposure relates to mental health outcomes. Participants reported their daily screen time in leisure activities like social media, gaming, streaming, along with indicators such as stress levels, sleep-related outcomes, and general mental wellness. The dataset is designed to explore whether higher screen exposure is associated with poorer mental wellbeing rather than focusing on specific platforms or interaction styles.\n",
    "\n",
    "Because all responses are collected at a single point in time, this dataset supports correlational analysis only. While we can observe patterns such as higher reported leisure screen time being associated with increased stress or reduced sleep quality, we cannot establish causal direction. For this project, Dataset 5 is used primarily as a supporting validation dataset, helping determine whether general screen exposure aligns with trends observed in social-media-specific analysis.\n",
    "\n",
    "Important metrics include leisure screen time, sleep duration and quality, stress level, and an overall mental wellbeing score capturing mood, energy, and focus. Together, these variables allow us to examine whether overall screen exposure, regardless of usage type, is meaningfully related to adult mental wellbeing and whether digital exposure has any measurable associaton with wellbeing in the first place.\n",
    "\n",
    "However, several potential biases must be considered. This dataset includes respondents aged 16-60, which requires some removal to align with out 18+ age demographics. Gender differences may also influence reported sleep, stress, and wellbeing and should be considered when interpreting results. Additionally, all measures are self-reported. This may introduce biases such as underreporting screen time or misjudging stress, sleep quality, and mental wellbeing.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "dataset_5 = pd.read_csv(\"data/00-raw/ScreenTime vs MentalWellness.csv\")\n",
    "dataset_5.head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dataset loads correctly but includes unnecessary column\n",
    "\n",
    "Remove empty column\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_5 = dataset_5.loc[:, ~dataset_5.columns.str.contains(\"^Unnamed\")]\n",
    "# Drops empty column and keeps only meaningful survey variable\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Count missing values in the dataset to be cleared.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_5.isnull().any()\n",
    "# No missing values to be handled\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Remove respondents under 18 to align with study demographics.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_5 = dataset_5[dataset_5[\"age\"] >= 18]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check for value limits.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_5.describe()\n",
    "# All data has correct min and max values.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Renaming columns for clarity.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_5 = dataset_5.rename(columns={\n",
    "    \"leisure_screen_hours\": \"Leisure Screen Time\",\n",
    "    \"sleep_hours\": \"Sleep Hours\",\n",
    "    \"sleep_quality_1_5\": \"Sleep Quality\",\n",
    "    \"stress_level_0_10\": \"Stress Level\",\n",
    "    \"mental_wellness_index_0_100\": \"Mental Wellness Index\"\n",
    "})\n",
    "# Improves readability without modifying any values, only labels.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Resetting index.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_5 = dataset_5.reset_index(drop=True)\n",
    "# This ensures clean sequential indexing after filtering.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Save processed dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_5.to_csv(\n",
    "    \"data/02-processed/ScreenTime_vs_MentalWellness_processed.csv\",\n",
    "    index=False\n",
    ")\n",
    "dataset_5.shape\n",
    "# Dataset is now clean and ready for analysis.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ethics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Instructions: REPLACE the contents of this cell with your work, including any updates to recover points lost in your proposal feedback"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Team Expectations "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Instructions: REPLACE the contents of this cell with your work, including any updates to recover points lost in your proposal feedback\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Project Timeline Proposal"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Instructions: Replace this with your timeline.  **PLEASE UPDATE your Timeline!** No battle plan survives contact with the enemy, so make sure we understand how your plans have changed.  Also if you have lost points on the previous checkpoint fix them"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  },
  "vscode": {
   "interpreter": {
    "hash": "16b860a9f5fc21240e9d88c0ee13691518c3ce67be252e54a03b9b5b11bd3c7a"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
