{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Rubric\n",
        "\n",
        "Instructions: DELETE this cell before you submit via a `git push` to your repo before deadline. This cell is for your reference only and is not needed in your report. \n",
        "\n",
        "Scoring: Out of 10 points\n",
        "\n",
        "- Each Developing  => -2 pts\n",
        "- Each Unsatisfactory/Missing => -4 pts\n",
        "  - until the score is \n",
        "\n",
        "If students address the detailed feedback in a future checkpoint they will earn these points back\n",
        "\n",
        "\n",
        "|                  | Unsatisfactory                                                                                                                                                                                                    | Developing                                                                                                                                                                                              | Proficient                                     | Excellent                                                                                                                              |\n",
        "|------------------|-------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|------------------------------------------------|----------------------------------------------------------------------------------------------------------------------------------------|\n",
        "| Data relevance   | Did not have data relevant to their question. Or the datasets don't work together because there is no way to line them up against each other. If there are multiple datasets, most of them have this trouble | Data was only tangentially relevant to the question or a bad proxy for the question. If there are multiple datasets, some of them may be irrelevant or can't be easily combined.                       | All data sources are relevant to the question. | Multiple data sources for each aspect of the project. It's clear how the data supports the needs of the project.                         |\n",
        "| Data description | Dataset or its cleaning procedures are not described. If there are multiple datasets, most have this trouble                                                                                              | Data was not fully described. If there are multiple datasets, some of them are not fully described                                                                                                      | Data was fully described                       | The details of the data descriptions and perhaps some very basic EDA also make it clear how the data supports the needs of the project. |\n",
        "| Data wrangling   | Did not obtain data. They did not clean/tidy the data they obtained.  If there are multiple datasets, most have this trouble                                                                                 | Data was partially cleaned or tidied. Perhaps you struggled to verify that the data was clean because they did not present it well. If there are multiple datasets, some have this trouble | The data is cleaned and tidied.                | The data is spotless and they used tools to visualize the data cleanliness and you were convinced at first glance                      |\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# COGS 108 - Data Checkpoint"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Authors\n",
        "\n",
        "Instructions: REPLACE the contents of this cell with your team list and their contributions. Note that this will change over the course of the checkpoints\n",
        "\n",
        "This is a modified [CRediT taxonomy of contributions](https://credit.niso.org). For each group member please list how they contributed to this project using these terms:\n",
        "> Analysis, Background research, Conceptualization, Data curation, Experimental investigation, Methodology, Project administration, Software, Visualization, Writing – original draft, Writing – review & editing\n",
        "\n",
        "Example team list and credits:\n",
        "- Alice Anderson: Conceptualization, Data curation, Methodology, Writing - original draft\n",
        "- Bob Barker:  Analysis, Software, Visualization\n",
        "- Charlie Chang: Project administration, Software, Writing - review & editing\n",
        "- Dani Delgado: Analysis, Background research, Visualization, Writing - original draft"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Research Question"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Instructions: REPLACE the contents of this cell with your work, including any updates to recover points lost in your proposal feedback\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Background and Prior Work"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Instructions: REPLACE the contents of this cell with your work, including any updates to recover points lost in your proposal feedback"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Hypothesis\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Instructions: REPLACE the contents of this cell with your work, including any updates to recover points lost in your proposal feedback\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Data overview\n",
        "\n",
        "Instructions: REPLACE the contents of this cell with descriptions of your actual datasets.\n",
        "\n",
        "For each dataset include the following information\n",
        "- Dataset #1\n",
        "  - Dataset Name:\n",
        "  - Link to the dataset:\n",
        "  - Number of observations:\n",
        "  - Number of variables:\n",
        "  - Description of the variables most relevant to this project\n",
        "  - Descriptions of any shortcomings this dataset has with repsect to the project\n",
        "- Dataset #2 (if you have more than one!)\n",
        "  - same as above\n",
        "- etc\n",
        "\n",
        "Each dataset deserves either a set of bullet points as above or a few sentences if you prefer that method.\n",
        "\n",
        "If you plan to use multiple datasets, add a few sentences about how you plan to combine these datasets.\n",
        "\n",
        "- Dataset #4\n",
        "  - Dataset Name: **Social Media and Mental Health** (survey; `smmh.csv`)\n",
        "  - Link to the dataset (official Kaggle page): `https://www.kaggle.com/datasets/souvikahmed071/social-media-and-mental-health`\n",
        "  - Reproducible raw CSV mirror used in this repo (no Kaggle login required): `https://raw.githubusercontent.com/Daerkns/correlation-between-social-media-and-mental-health/main/smmh.csv`\n",
        "  - Number of observations: **481** survey responses (rows)\n",
        "  - Number of variables: **21** columns\n",
        "  - Description of the variables most relevant to this project:\n",
        "    - Demographics/controls: age, gender, relationship status, occupation status, affiliated organization type\n",
        "    - Social media usage: whether they use social media, which platforms (comma-separated list), and average time spent per day (categorical bins)\n",
        "    - Wellbeing / mental health proxies (mostly Likert 1–5): distractedness, worries, concentration difficulty, social comparison frequency + feelings about comparisons, validation seeking, feeling depressed/down, interest fluctuation, sleep issues\n",
        "  - Shortcomings / limitations:\n",
        "    - Cross-sectional and self-reported survey data (no causal conclusions; subject to recall/social desirability bias)\n",
        "    - Does not include direct “active interaction” counts (posting/commenting/DMs); we will treat it as a **supporting** dataset and use available proxies (time-spent, platforms, comparison/validation items)\n",
        "    - Potential sample bias (survey respondents are not representative of all adults)\n",
        "    - Small number of potentially inconsistent responses (e.g., `Do you use social media? = No` but other usage questions answered) that we will flag and handle explicitly in cleaning"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Run this code every time when you're actively developing modules in .py files.  It's not needed if you aren't making modules\n",
        "#\n",
        "## this code is necessary for making sure that any modules we load are updated here \n",
        "## when their source code .py files are modified\n",
        "\n",
        "%load_ext autoreload\n",
        "%autoreload 2"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Setup code -- this only needs to be run once after cloning the repo!\n",
        "# This cell downloads raw data into `data/00-raw/`.\n",
        "\n",
        "# If you're missing packages in your environment, uncomment this line\n",
        "# %pip install pandas numpy matplotlib seaborn\n",
        "\n",
        "import os\n",
        "import urllib.request\n",
        "\n",
        "# Ensure expected data directories exist.\n",
        "os.makedirs('data/00-raw', exist_ok=True)\n",
        "os.makedirs('data/01-interim', exist_ok=True)\n",
        "os.makedirs('data/02-processed', exist_ok=True)\n",
        "\n",
        "# Dataset #4 (Social Media and Mental Health)\n",
        "SMMH_URL = 'https://raw.githubusercontent.com/Daerkns/correlation-between-social-media-and-mental-health/main/smmh.csv'\n",
        "SMMH_LOCAL = 'data/00-raw/smmh.csv'\n",
        "\n",
        "def download_if_missing(url: str, local_path: str) -> None:\n",
        "    if os.path.exists(local_path) and os.path.getsize(local_path) > 0:\n",
        "        print(f\"Already exists: {local_path}\")\n",
        "        return\n",
        "\n",
        "    print(f\"Downloading to: {local_path}\")\n",
        "    urllib.request.urlretrieve(url, local_path)\n",
        "    print(f\"Downloaded: {local_path} ({os.path.getsize(local_path)} bytes)\")\n",
        "\n",
        "download_if_missing(SMMH_URL, SMMH_LOCAL)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Dataset #1 \n",
        "\n",
        "Instructions: \n",
        "1. Change the header from Dataset #1 to something more descriptive of the dataset\n",
        "2. Write a few paragraphs about this dataset. Make sure to cover\n",
        "   1. Describe the important metrics, what units they are in, and giv some sense of what they mean.  For example \"Fasting blood glucose in units of mg glucose per deciliter of blood.  Normal values for healthy individuals range from 70 to 100 mg/dL.  Values 100-125 are prediabetic and values >125mg/dL indicate diabetes. Values <70 indicate hypoglycemia. Fasting idicates the patient hasn't eaten in the last 8 hours.  If blood glucose is >250 or <50 at any time (regardless of the time of last meal) the patient's life may be in immediate danger\"\n",
        "   2. If there are any major concerns with the dataset, describe them. For example \"Dataset is composed of people who are serious enough about eating healthy that they voluntarily downloaded an app dedicated to tracking their eating patterns. This sample is likely biased because of that self-selection. These people own smartphones and may be healthier and may have more disposable income than the average person.  Those who voluntarily log conscientiously and for long amounts of time are also likely even more interested in health than those who download the app and only log a bit before getting tired of it\"\n",
        "3. Use the cell below to \n",
        "    1. load the dataset \n",
        "    2. make the dataset tidy or demonstrate that it was already tidy\n",
        "    3. demonstrate the size of the dataset\n",
        "    4. find out how much data is missing, where its missing, and if its missing at random or seems to have any systematic relationships in its missingness\n",
        "    5. find and flag any outliers or suspicious entries\n",
        "    6. clean the data or demonstrate that it was already clean.  You may choose how to deal with missingness (dropna of fillna... how='any' or 'all') and you should justify your choice in some way\n",
        "    7. You will load raw data from `data/00-raw/`, you will (optionally) write intermediate stages of your work to `data/01-interim` and you will write the final fully wrangled version of your data to `data/02-processed`\n",
        "4. Optionally you can also show some summary statistics for variables that you think are important to the project\n",
        "5. Feel free to add more cells here if that's helpful for you\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "## Dataset #1 wrangling (Social Media User Activity Dataset)\n",
        "from pathlib import Path\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import re\n",
        "\n",
        "RAW_DIR = Path(\"data/00-raw\")\n",
        "INTERIM_DIR = Path(\"data/01-interim\")\n",
        "PROCESSED_DIR = Path(\"data/02-processed\")\n",
        "\n",
        "RAW_DIR.mkdir(parents=True, exist_ok=True)\n",
        "INTERIM_DIR.mkdir(parents=True, exist_ok=True)\n",
        "PROCESSED_DIR.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "csv_path = RAW_DIR / \"instagram_usage_lifestyle.csv\"\n",
        "if not csv_path.exists():\n",
        "    all_names = [p.name for p in sorted(RAW_DIR.glob(\"*.csv\"))]\n",
        "    raise FileNotFoundError(\n",
        "        f\"Expected file '{csv_path.name}' not found in data/00-raw/. \"\n",
        "        f\"Current CSV files: {all_names}\"\n",
        "    )\n",
        "\n",
        "df_raw = pd.read_csv(csv_path)\n",
        "\n",
        "print(f\"Using file: {csv_path.name}\")\n",
        "print(f\"Shape: {df_raw.shape}\")\n",
        "print(\"\\nColumns:\")\n",
        "for c in df_raw.columns:\n",
        "    print(\"-\", c)\n",
        "\n",
        "display(df_raw.head())\n",
        "\n",
        "# ---------- Data quality checks ----------\n",
        "df = df_raw.copy()\n",
        "\n",
        "print(\"\\nRows, columns:\", df.shape)\n",
        "print(\"Duplicate rows:\", df.duplicated().sum())\n",
        "\n",
        "missing = (\n",
        "    df.isna()\n",
        "    .sum()\n",
        "    .to_frame(\"missing_count\")\n",
        "    .assign(missing_pct=lambda x: (x[\"missing_count\"] / len(df) * 100).round(2))\n",
        "    .sort_values(\"missing_count\", ascending=False)\n",
        ")\n",
        "print(\"\\nTop missingness columns:\")\n",
        "display(missing.head(25))\n",
        "\n",
        "print(\"\\nData types:\")\n",
        "display(df.dtypes.to_frame(\"dtype\"))\n",
        "\n",
        "def suggest_columns(columns, keywords, regex=False):\n",
        "    out = []\n",
        "    for c in columns:\n",
        "        cl = str(c).lower()\n",
        "        if regex:\n",
        "            if any(re.search(k, cl) for k in keywords):\n",
        "                out.append(c)\n",
        "        else:\n",
        "            if any(k in cl for k in keywords):\n",
        "                out.append(c)\n",
        "    return out\n",
        "\n",
        "candidate_map = {\n",
        "    \"age\": suggest_columns(df.columns, [r\"(^|[_\\s\\-])age([_\\s\\-]|$)\"], regex=True),\n",
        "    \"passive_use\": suggest_columns(df.columns, [\"passive\", \"scroll\", \"browse\", \"view\", \"watch\", \"consume\", \"time\", \"minutes\"]),\n",
        "    \"active_use\": suggest_columns(df.columns, [\"active\", \"post\", \"comment\", \"like\", \"message\", \"dm\", \"interact\"]),\n",
        "    \"stress\": suggest_columns(df.columns, [\"stress\", \"anxiety\", \"anx\"]),\n",
        "    \"happiness\": suggest_columns(df.columns, [\"happiness\", \"happy\", \"wellbeing\", \"well-being\", \"mood\"]),\n",
        "}\n",
        "\n",
        "print(\"\\nCandidate columns by concept:\")\n",
        "for k, v in candidate_map.items():\n",
        "    print(f\"\\n{k}:\")\n",
        "    if v:\n",
        "        for name in v:\n",
        "            print(\" -\", name)\n",
        "    else:\n",
        "        print(\" - (no obvious match found)\")\n",
        "\n",
        "# ---------- Cleaning config ----------\n",
        "col_map = {\n",
        "    \"age\": \"age\",\n",
        "    \"passive_use\": \"time_on_feed_per_day\",\n",
        "    \"active_use\": \"posts_created_per_week\",\n",
        "    \"stress\": \"perceived_stress_score\",\n",
        "    \"happiness\": \"self_reported_happiness\",\n",
        "}\n",
        "\n",
        "control_cols = [\n",
        "    \"gender\",\n",
        "    \"country\",\n",
        "    \"urban_rural\",\n",
        "    \"income_level\",\n",
        "    \"employment_status\",\n",
        "    \"education_level\",\n",
        "    \"exercise_hours_per_week\",\n",
        "    \"sleep_hours_per_night\",\n",
        "    \"weekly_work_hours\",\n",
        "]\n",
        "\n",
        "\n",
        "def to_snake(s):\n",
        "    s = str(s).strip().lower()\n",
        "    s = re.sub(r\"[^a-z0-9]+\", \"_\", s)\n",
        "    s = re.sub(r\"_+\", \"_\", s).strip(\"_\")\n",
        "    return s\n",
        "\n",
        "\n",
        "df_clean = df.copy()\n",
        "df_clean = df_clean.rename(columns={c: to_snake(c) for c in df_clean.columns})\n",
        "\n",
        "mapped = {k: (to_snake(v) if v is not None else None) for k, v in col_map.items()}\n",
        "control_cols_snake = [to_snake(c) for c in control_cols]\n",
        "\n",
        "selected_cols = [v for v in mapped.values() if v is not None] + control_cols_snake\n",
        "selected_cols = [c for c in selected_cols if c in df_clean.columns]\n",
        "\n",
        "if selected_cols:\n",
        "    df_clean = df_clean[selected_cols].copy()\n",
        "else:\n",
        "    print(\"\\nNo col_map/control_cols selected yet. Keeping all columns for now.\")\n",
        "\n",
        "for key in [\"age\", \"passive_use\", \"active_use\", \"stress\", \"happiness\"]:\n",
        "    col = mapped.get(key)\n",
        "    if col is not None and col in df_clean.columns:\n",
        "        df_clean[col] = pd.to_numeric(df_clean[col], errors=\"coerce\")\n",
        "\n",
        "if mapped[\"age\"] is not None and mapped[\"age\"] in df_clean.columns:\n",
        "    before = len(df_clean)\n",
        "    df_clean = df_clean[df_clean[mapped[\"age\"]] >= 18]\n",
        "    print(f\"Age filter (18+): {before} -> {len(df_clean)} rows\")\n",
        "else:\n",
        "    print(\"Age column not mapped; 18+ filter skipped.\")\n",
        "\n",
        "before_dup = len(df_clean)\n",
        "df_clean = df_clean.drop_duplicates()\n",
        "print(f\"Drop duplicates: {before_dup} -> {len(df_clean)} rows\")\n",
        "\n",
        "required_cols = [\n",
        "    mapped[k]\n",
        "    for k in [\"passive_use\", \"active_use\", \"stress\", \"happiness\"]\n",
        "    if mapped[k] is not None and mapped[k] in df_clean.columns\n",
        "]\n",
        "if required_cols:\n",
        "    before_req = len(df_clean)\n",
        "    df_clean = df_clean.dropna(subset=required_cols)\n",
        "    print(f\"Drop NA in required vars: {before_req} -> {len(df_clean)} rows\")\n",
        "else:\n",
        "    print(\"Required vars not fully mapped yet; NA filtering skipped.\")\n",
        "\n",
        "numeric_cols = df_clean.select_dtypes(include=[np.number]).columns.tolist()\n",
        "outlier_counts = {}\n",
        "for c in numeric_cols:\n",
        "    s = df_clean[c].dropna()\n",
        "    if len(s) < 5:\n",
        "        outlier_counts[c] = 0\n",
        "        continue\n",
        "    q1, q3 = s.quantile(0.25), s.quantile(0.75)\n",
        "    iqr = q3 - q1\n",
        "    if iqr == 0:\n",
        "        outlier_counts[c] = 0\n",
        "        continue\n",
        "    lo, hi = q1 - 1.5 * iqr, q3 + 1.5 * iqr\n",
        "    outlier_counts[c] = int(((df_clean[c] < lo) | (df_clean[c] > hi)).sum())\n",
        "\n",
        "outlier_report = pd.Series(outlier_counts, name=\"outlier_count\").sort_values(ascending=False).to_frame()\n",
        "print(\"\\nOutlier counts (IQR):\")\n",
        "display(outlier_report.head(20))\n",
        "\n",
        "interim_path = INTERIM_DIR / \"social_media_user_activity_interim.csv\"\n",
        "processed_path = PROCESSED_DIR / \"social_media_user_activity_cleaned.csv\"\n",
        "\n",
        "df_clean.to_csv(interim_path, index=False)\n",
        "df_clean.to_csv(processed_path, index=False)\n",
        "\n",
        "print(\"\\nFinal cleaned shape:\", df_clean.shape)\n",
        "print(\"Saved interim:\", interim_path)\n",
        "print(\"Saved processed:\", processed_path)\n",
        "display(df_clean.head())\n",
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Dataset #2 \n",
        "\n",
        "See instructions above for Dataset #1.  Feel free to keep adding as many more datasets as you need.  Put each new dataset in its own section just like these. \n",
        "\n",
        "Lastly if you do have multiple datasets, add another section where you demonstrate how you will join, align, cross-reference or whatever to combine data from the different datasets\n",
        "\n",
        "Please note that you can always keep adding more datasets in the future if these datasets you turn in for the checkpoint aren't sufficient.  The goal here is demonstrate that you can obtain and wrangle data.  You are not tied down to only use what you turn in right now."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "## YOUR CODE TO LOAD/CLEAN/TIDY/WRANGLE THE DATA GOES HERE\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Dataset #4: Social Media and Mental Health (`smmh.csv`)\n",
        "\n",
        "**Official dataset page (citation):** `https://www.kaggle.com/datasets/souvikahmed071/social-media-and-mental-health`\n",
        "\n",
        "**How we download it reproducibly for this repo:** Kaggle downloads require login/terms acceptance, so for reproducibility we download the same `smmh.csv` from a public raw mirror into `data/00-raw/` using the setup cell above.\n",
        "\n",
        "Each row is one survey response (timestamped). Columns include demographics, social media usage (platforms + time-spent category), and several mental-health / wellbeing proxy items (mostly Likert 1–5).\n",
        "\n",
        "In the wrangling below we:\n",
        "- Rename verbose survey-question column names to short `snake_case` names\n",
        "- Parse timestamps and coerce numeric Likert items\n",
        "- Standardize key categorical values (e.g., gender labels)\n",
        "- Engineer a numeric **hours/day** proxy from the time-spent categories\n",
        "- Expand the comma-separated platforms field into a small set of indicator columns\n",
        "- Check missingness, outliers, and a small set of internal inconsistencies\n",
        "- Save a processed dataset to `data/02-processed/smmh_processed.csv`"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "raw_path = 'data/00-raw/smmh.csv'\n",
        "\n",
        "# Load raw data (downloaded in the setup cell)\n",
        "df_raw = pd.read_csv(raw_path, skipinitialspace=True)\n",
        "df_raw.columns = [c.strip() for c in df_raw.columns]\n",
        "\n",
        "rename_map = {\n",
        "    'Timestamp': 'timestamp',\n",
        "    '1. What is your age?': 'age',\n",
        "    '2. Gender': 'gender',\n",
        "    '3. Relationship Status': 'relationship_status',\n",
        "    '4. Occupation Status': 'occupation_status',\n",
        "    '5. What type of organizations are you affiliated with?': 'organization_affiliation',\n",
        "    '6. Do you use social media?': 'uses_social_media_answer',\n",
        "    '7. What social media platforms do you commonly use?': 'platforms',\n",
        "    '8. What is the average time you spend on social media every day?': 'avg_daily_time',\n",
        "    '9. How often do you find yourself using Social media without a specific purpose?': 'sm_no_purpose_freq',\n",
        "    '10. How often do you get distracted by Social media when you are busy doing something?': 'sm_distracted_freq',\n",
        "    \"11. Do you feel restless if you haven't used Social media in a while?\": 'sm_restless_freq',\n",
        "    '12. On a scale of 1 to 5, how easily distracted are you?': 'easily_distracted',\n",
        "    '13. On a scale of 1 to 5, how much are you bothered by worries?': 'bothered_by_worries',\n",
        "    '14. Do you find it difficult to concentrate on things?': 'difficulty_concentrating',\n",
        "    '15. On a scale of 1-5, how often do you compare yourself to other successful people through the use of social media?': 'compare_to_successful_freq',\n",
        "    '16. Following the previous question, how do you feel about these comparisons, generally speaking?': 'feel_about_comparisons',\n",
        "    '17. How often do you look to seek validation from features of social media?': 'seek_validation_freq',\n",
        "    '18. How often do you feel depressed or down?': 'depressed_or_down_freq',\n",
        "    '19. On a scale of 1 to 5, how frequently does your interest in daily activities fluctuate?': 'interest_fluctuation',\n",
        "    '20. On a scale of 1 to 5, how often do you face issues regarding sleep?': 'sleep_issues',\n",
        "}\n",
        "\n",
        "missing_cols = sorted(set(rename_map) - set(df_raw.columns))\n",
        "if missing_cols:\n",
        "    raise ValueError(f\"Unexpected raw columns (missing expected): {missing_cols}\")\n",
        "\n",
        "# Rename columns\n",
        "df = df_raw.rename(columns=rename_map).copy()\n",
        "\n",
        "# Parse timestamp + age\n",
        "df['timestamp'] = pd.to_datetime(df['timestamp'], errors='coerce')\n",
        "df['age'] = pd.to_numeric(df['age'], errors='coerce')\n",
        "\n",
        "# Standardize string columns\n",
        "string_cols = [\n",
        "    'gender',\n",
        "    'relationship_status',\n",
        "    'occupation_status',\n",
        "    'organization_affiliation',\n",
        "    'uses_social_media_answer',\n",
        "    'platforms',\n",
        "    'avg_daily_time',\n",
        "    'feel_about_comparisons',\n",
        "]\n",
        "for c in string_cols:\n",
        "    df[c] = df[c].astype('string').str.strip()\n",
        "\n",
        "# Normalize gender labels (keep it simple + transparent)\n",
        "def _normalize_gender(x):\n",
        "    if pd.isna(x):\n",
        "        return pd.NA\n",
        "    s = str(x).strip()\n",
        "    s_low = s.lower()\n",
        "\n",
        "    if s_low in {'nb', 'nonbinary', 'non-binary'}:\n",
        "        return 'Non-binary'\n",
        "    if s_low in {'unsure'}:\n",
        "        return 'Unsure'\n",
        "    if s_low in {'trans'}:\n",
        "        return 'Trans'\n",
        "\n",
        "    # Common values like 'Male'/'Female' stay as-is (titlecased)\n",
        "    return s.title()\n",
        "\n",
        "df['gender'] = df['gender'].map(_normalize_gender)\n",
        "\n",
        "# Coerce Likert-style items to numeric\n",
        "likert_cols = [\n",
        "    'sm_no_purpose_freq',\n",
        "    'sm_distracted_freq',\n",
        "    'sm_restless_freq',\n",
        "    'easily_distracted',\n",
        "    'bothered_by_worries',\n",
        "    'difficulty_concentrating',\n",
        "    'compare_to_successful_freq',\n",
        "    'seek_validation_freq',\n",
        "    'depressed_or_down_freq',\n",
        "    'interest_fluctuation',\n",
        "    'sleep_issues',\n",
        "]\n",
        "for c in likert_cols:\n",
        "    df[c] = pd.to_numeric(df[c], errors='coerce').astype('Int64')\n",
        "\n",
        "print('Raw shape:', df_raw.shape)\n",
        "print('Renamed/typed shape:', df.shape)\n",
        "df.head(3)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# --- Feature engineering: platforms + time spent ---\n",
        "\n",
        "from IPython.display import display\n",
        "\n",
        "# Convert Yes/No to a boolean indicator (keep original + a cleaned version)\n",
        "df['uses_social_media_original'] = (\n",
        "    df['uses_social_media_answer']\n",
        "      .str.lower()\n",
        "      .map({'yes': True, 'no': False})\n",
        "      .astype('boolean')\n",
        ")\n",
        "\n",
        "common_platforms = [\n",
        "    'Facebook', 'Instagram', 'Twitter', 'YouTube',\n",
        "    'TikTok', 'Snapchat', 'Discord', 'Reddit', 'Pinterest'\n",
        "]\n",
        "_common_set = set(common_platforms)\n",
        "\n",
        "def _normalize_platform(p: str) -> str:\n",
        "    p = p.strip()\n",
        "    p_low = p.lower()\n",
        "\n",
        "    mapping = {\n",
        "        'youtube': 'YouTube',\n",
        "        'tik tok': 'TikTok',\n",
        "        'tiktok': 'TikTok',\n",
        "        'fb': 'Facebook',\n",
        "        'x': 'Twitter',\n",
        "    }\n",
        "    if p_low in mapping:\n",
        "        return mapping[p_low]\n",
        "\n",
        "    # Keep capitalization consistent for common platforms\n",
        "    for canon in common_platforms:\n",
        "        if p_low == canon.lower():\n",
        "            return canon\n",
        "\n",
        "    return p.title()\n",
        "\n",
        "def _parse_platforms(x):\n",
        "    if pd.isna(x):\n",
        "        return []\n",
        "    items = [i.strip() for i in str(x).split(',') if i.strip()]\n",
        "    norm = [_normalize_platform(i) for i in items]\n",
        "\n",
        "    # De-duplicate while preserving order\n",
        "    out = []\n",
        "    seen = set()\n",
        "    for p in norm:\n",
        "        if p not in seen:\n",
        "            out.append(p)\n",
        "            seen.add(p)\n",
        "    return out\n",
        "\n",
        "platform_list = df['platforms'].map(_parse_platforms)\n",
        "df['platform_count'] = platform_list.map(len)\n",
        "df['platforms_clean'] = platform_list.map(lambda lst: ', '.join(lst) if lst else pd.NA).astype('string')\n",
        "\n",
        "for p in common_platforms:\n",
        "    col = f\"uses_{p.lower()}\".replace(' ', '_')\n",
        "    df[col] = platform_list.map(lambda lst, p=p: p in lst).astype('boolean')\n",
        "\n",
        "df['uses_other_platform'] = platform_list.map(lambda lst: any(p not in _common_set for p in lst)).astype('boolean')\n",
        "\n",
        "# Map time-spent bins to an approximate numeric hours/day proxy.\n",
        "# Midpoints are used for closed intervals; \"More than 5 hours\" is set to 5.5 as a conservative constant.\n",
        "time_to_hours = {\n",
        "    'Less than an Hour': 0.5,\n",
        "    'Between 1 and 2 hours': 1.5,\n",
        "    'Between 2 and 3 hours': 2.5,\n",
        "    'Between 3 and 4 hours': 3.5,\n",
        "    'Between 4 and 5 hours': 4.5,\n",
        "    'More than 5 hours': 5.5,\n",
        "}\n",
        "df['avg_daily_hours'] = df['avg_daily_time'].map(time_to_hours).astype('Float64')\n",
        "\n",
        "# --- Data quality checks: missingness, outliers, inconsistencies ---\n",
        "\n",
        "print('\\nAge range (min/max):', df['age'].min(), df['age'].max())\n",
        "print('Age quantiles:')\n",
        "print(df['age'].quantile([0, 0.01, 0.25, 0.5, 0.75, 0.99, 1]).to_string())\n",
        "\n",
        "# Flag ages that are suspicious for an \"adults 18+\" research question\n",
        "suspicious_age_mask = (df['age'] < 18) | (df['age'] > 100)\n",
        "print('\\nSuspicious ages (<18 or >100):', int(suspicious_age_mask.sum()))\n",
        "if suspicious_age_mask.any():\n",
        "    display(df.loc[suspicious_age_mask, ['timestamp', 'age', 'gender']].head(10))\n",
        "\n",
        "# Missingness summary\n",
        "missing = df.isna().sum().sort_values(ascending=False)\n",
        "missing_pct = (df.isna().mean() * 100).round(2)\n",
        "missing_table = pd.DataFrame({'missing_count': missing, 'missing_pct': missing_pct}).query('missing_count > 0')\n",
        "print('\\nMissingness (only columns with missing values):')\n",
        "display(missing_table)\n",
        "\n",
        "# Inconsistency: answered \"No\" to using social media but still provided platforms or time spent\n",
        "inferred_use = (df['platform_count'] > 0) | df['avg_daily_hours'].notna()\n",
        "inconsistent = (df['uses_social_media_original'] == False) & inferred_use\n",
        "print('\\nInconsistent rows (uses_social_media_original == False but usage fields present):', int(inconsistent.sum()))\n",
        "if inconsistent.any():\n",
        "    display(df.loc[inconsistent, ['uses_social_media_answer', 'platforms', 'avg_daily_time', 'platform_count']].head(10))\n",
        "\n",
        "# Cleaning rule: keep the original answer, but also create a \"clean\" usage boolean that\n",
        "# treats any reported usage fields as evidence of actual use.\n",
        "df['uses_social_media'] = (\n",
        "    df['uses_social_media_original']\n",
        "      .fillna(False)\n",
        "      .astype('boolean')\n",
        "    | inferred_use.astype('boolean')\n",
        ")\n",
        "\n",
        "# Check Likert ranges\n",
        "likert_minmax = df[likert_cols].agg(['min', 'max']).T\n",
        "out_of_range = (df[likert_cols] < 1) | (df[likert_cols] > 5)\n",
        "print('\\nLikert min/max (should be within 1–5):')\n",
        "display(likert_minmax)\n",
        "print('Out-of-range Likert entries (count):', int(out_of_range.sum().sum()))\n",
        "\n",
        "# --- Save processed dataset ---\n",
        "\n",
        "processed_cols = [\n",
        "    'timestamp', 'age', 'gender', 'relationship_status', 'occupation_status', 'organization_affiliation',\n",
        "    'uses_social_media_original', 'uses_social_media',\n",
        "    'avg_daily_time', 'avg_daily_hours',\n",
        "    'platforms_clean', 'platform_count',\n",
        "] + [\n",
        "    f\"uses_{p.lower()}\".replace(' ', '_') for p in common_platforms\n",
        "] + [\n",
        "    'uses_other_platform',\n",
        "    'sm_no_purpose_freq', 'sm_distracted_freq', 'sm_restless_freq',\n",
        "    'easily_distracted', 'bothered_by_worries', 'difficulty_concentrating',\n",
        "    'compare_to_successful_freq', 'feel_about_comparisons', 'seek_validation_freq',\n",
        "    'depressed_or_down_freq', 'interest_fluctuation', 'sleep_issues'\n",
        "]\n",
        "\n",
        "processed_path = 'data/02-processed/smmh_processed.csv'\n",
        "df_processed = df[processed_cols].copy()\n",
        "df_processed.to_csv(processed_path, index=False)\n",
        "\n",
        "print(f\"\\nSaved processed dataset to: {processed_path}\")\n",
        "print('Processed shape:', df_processed.shape)\n",
        "df_processed.head(3)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# --- Minimal EDA (sanity-check visuals) ---\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "sns.set_theme(style='whitegrid')\n",
        "\n",
        "# Distribution of time spent (numeric proxy)\n",
        "plt.figure(figsize=(7, 4))\n",
        "sns.histplot(df_processed['avg_daily_hours'].dropna().astype(float), bins=12)\n",
        "plt.title('Approx. Daily Social Media Use (hours/day)')\n",
        "plt.xlabel('avg_daily_hours')\n",
        "plt.ylabel('count')\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "# Gender distribution\n",
        "plt.figure(figsize=(7, 4))\n",
        "order = df_processed['gender'].value_counts(dropna=False).index\n",
        "sns.countplot(data=df_processed, y='gender', order=order)\n",
        "plt.title('Gender distribution')\n",
        "plt.xlabel('count')\n",
        "plt.ylabel('gender')\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "# Relationship between time spent and sleep issues (Likert 1–5)\n",
        "eda_tmp = df_processed.dropna(subset=['avg_daily_hours', 'sleep_issues']).copy()\n",
        "eda_tmp['avg_daily_hours'] = eda_tmp['avg_daily_hours'].astype(float)\n",
        "eda_tmp['sleep_issues'] = pd.to_numeric(eda_tmp['sleep_issues'], errors='coerce')\n",
        "\n",
        "plt.figure(figsize=(7, 4))\n",
        "sns.regplot(\n",
        "    data=eda_tmp,\n",
        "    x='avg_daily_hours',\n",
        "    y='sleep_issues',\n",
        "    x_jitter=0.05,\n",
        "    y_jitter=0.05,\n",
        "    scatter_kws={'alpha': 0.25, 's': 18},\n",
        "    line_kws={'color': 'black'}\n",
        ")\n",
        "plt.title('Time on Social Media vs Sleep Issues (proxy)')\n",
        "plt.xlabel('avg_daily_hours')\n",
        "plt.ylabel('sleep_issues (1–5)')\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Ethics"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Instructions: REPLACE the contents of this cell with your work, including any updates to recover points lost in your proposal feedback"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Team Expectations "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Instructions: REPLACE the contents of this cell with your work, including any updates to recover points lost in your proposal feedback\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Project Timeline Proposal"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Instructions: Replace this with your timeline.  **PLEASE UPDATE your Timeline!** No battle plan survives contact with the enemy, so make sure we understand how your plans have changed.  Also if you have lost points on the previous checkpoint fix them"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.13"
    },
    "vscode": {
      "interpreter": {
        "hash": "16b860a9f5fc21240e9d88c0ee13691518c3ce67be252e54a03b9b5b11bd3c7a"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4
}