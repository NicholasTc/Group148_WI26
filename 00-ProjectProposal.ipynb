{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# COGS 108 - Project Proposal\n",
    "\n",
    "## Authors"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Evan Honggo Widjojo: Conceptualization, Methodology\n",
    "- Ahmad Bin Feizal: Background research\n",
    "- Nicholas Chan: Data curation\n",
    "- Fadi Gorgees: Background research, Methodology\n",
    "- Neenos Yaldiko: Project administration\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Research Question"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How are different styles of social media use, passive consumption versus active interaction, associated with wellbeing outcomes among adults aged 18+? We define passive consumption as behaviors like time spent browsing content and viewing posts or short form videos, and active interaction as behaviors like posting, commenting, liking, and direct messaging. Our primary wellbeing outcomes are self reported stress and self reported happiness, and we will also examine sleep related and basic physical health indicators when available. To reduce confounding in this observational study, we will control for demographic factors, socioeconomic status, and lifestyle variables such as physical activity and work hours. We will use regression based statistical inference to estimate associations and will avoid causal claims."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Background and Prior Work"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Social media uses are now a major component of everyday life. Instagram, Facebook, TikTok and Snapchat have accrued daily engagement of the majority of adults. As social media use has grown, research has shifted towards focusing on the manner in which individuals engage with it. For this study, we are using a widely adopted distinction which identifies active interaction as including posting content, commenting, liking, and direct messaging. On the other hand, passive consumption includes browsing feeds or watching short-term videos Our initial research suggests that these usage styles may influence different psychological mechanisms with passive use often linked to social comparison and envy, while active use may have higher perceived support and social connectedness. However, associations with other wellbeing outcomes including stress, happiness, sleep quality, and physical health remain mixed and motivates further investigation.\n\nOne prior influential effort in this study is the Social Media Activity Questionnaire (SMAQ), which surveyed 1,230 participants derived from Facebook Activity Questionnaire. This study by Ozimek, Brailovskaia and Bierhoff separated actions identified as active and passive engagements with a rating scale from 1 to 5.<a name=\"cite_ref-1\"></a>[<sup>1</sup>](#cite_note-1) With the application of exploratory factor analysis (EFA) to identify behavioral dimensions, outcomes regarding mental health were assessed using established scales. These include Bergen Social Media Addiction Scale, Fear of Missing Out (FoMO) scale, and the Depression Anxiety Stress Scales (DASS). In contrast to our project, this study focused on negative mental health indicators, rather than trying to estimate well being directly which remains  our priority. Research outcome displayed stronger association between active use depression, anxiety and stress, while passive was more strongly associated with problematic behavioral tendencies like addiction and FoMO. Researchers' opinions attributed these tendencies to upwards social comparison and envy enabled by consumed contents.\n\nOne notable limitation to the SMAQ study was the disproportionate sample of mostly young and females, which limits generalizability to the broader population. Other than that, the data set was limited to Facebook users and geographically restricted to the Ruhr region of Germany. This makes it difficult to extrapolate any observations to older adults, different cultural contexts, and our specific social media of choice, Instagram.\n\n Another referenced perspective is provided by the study “Are active and passive social media use related to mental health, wellbeing, and social support outcomes?”. This meta-analysis synthesizes findings from 141 quantitative studies which studies correlations between social media use styles and 13 mental health and wellbeing outcomes. Using pooled effect sizes, Godard and Holtzman report that neither active nor passive use strongly predicts well being on average, with most effects being minute.<a name=\"cite_ref-2\"></a>[<sup>2</sup>](#cite_note-2) Active use has modest association with perceived social support and wellbeing, but also with slightly higher anxiety. Passive use showed overall weak associations but were more linked to worse emotional outcomes in general contexts. Crucially, this study highlights that demographic characteristics and contextual moderators, such as age and usage setting, have more substantial influence on outcomes. These findings show the importance of controlling for confounding factors rather than simplistic narratives like “active is good, passive is bad” which becomes a focus in guiding our project.\n\n A study that heavily grounded age as contextual moderator is Underwood’s expanded literature review which is specific to adolescents aged 10-19. Reviewing 16 peer-reviewed studies gathered from MEDLINE and PubMed via keyword search, the literature review concludes that both active and passive social media has positive association with negative mental health outcomes in adolescents.<a name=\"cite_ref-3\"></a>[<sup>3</sup>](#cite_note-3) While evidence was insufficient to conclude that one style is categorically more harmful, this study shows the repeated importance of grounding research by age as observations of effects varied across different stages of adolescence. Our current study aims to control moderating factors by narrowing our analysis to Instagram usage among adults aged 18+. We target to examine both positive and negative wellbeing outcomes stretching beyond happiness and stress to include physical health factors like sleep and physical health while explicitly accounting for demographic, socioeconomic, and lifestyle confounders.\n",
    "1. <a name=\"cite_note-1\"></a> [^](#cite_ref-1)  Ozimek, P., Brailovskaia, J., Bierhoff, H-W. (2023) Active and passive behavior in social media: Validating the Social Media Activity Questionnaire (SMAQ), Telematics and Informatics Reports, Volume 10, 100048, ISSN 2772-5030, https://doi.org/10.1016/j.teler.2023.100048\n",
    "2. <a name=\"cite_note-2\"></a> [^](#cite_ref-2) Godard, R., Holtzman, S. (2024). Are active and passive social media use related to mental health, wellbeing, and social support outcomes? A meta-analysis of 141 studies, Journal of Computer-Mediated Communication, Volume 29, Issue 1, January 2024, zmad055, https://doi.org/10.1093/jcmc/zmad055\n",
    "3. <a name=\"cite_note-3\"></a> [^](#cite_ref-3)  Underwood, L. (2024). Difference Between the Impact of Active Social Media Use and Passive Social Media Use on Adolescent Mental Health: An Expanded Literature Review. The Eleanor Mann School of Nursing Undergraduate Honors. https://scholarworks.uark.edu/nursuht/211"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hypothesis\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We hypothesize that among adults aged 18+, higher passive social media consumption will be associated with higher self reported stress and lower self reported happiness, while higher active interaction will be associated with lower stress and higher happiness. This is because passive browsing tends to increase social comparison and rumination, whereas active interaction is more likely to involve social connection and support, which can buffer stress and improve mood."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
  "# Data\n\n## 1) Ideal dataset\n\n### Unit of observation\n- One row per person, ideally with a time window attached.\n- Best case: repeated measures (same person measured multiple times), because it helps us separate “who they are” from “what they did this week.”\n\n### Variables we need (and why)\n\n#### Unique ID + timestamps\n- `user_id` (unique person ID)\n- `date` or `week_id` (so we know the time window)\n\n#### Main predictors (social media use style)\n- Passive consumption (examples): minutes scrolling/browsing, number of posts viewed, short-form video viewing time\n- Active interaction (examples): number of posts made, comments written, likes given, DMs/messages sent\n- Ideally we also have platform (TikTok/IG/X/Reddit/etc.), because usage meaning can differ by platform.\n\n#### Outcome variables (wellbeing)\n- Primary: self-reported stress, self-reported happiness\n- Optional if available: sleep (hours, sleep quality), basic physical health (self-rated health, fatigue), maybe mood indicators\n\n#### Controls (to reduce confounding)\n- Demographics: age, gender, education\n- Socioeconomic: income or proxy, employment status\n- Lifestyle: physical activity, work hours, student status (if relevant)\n- Optional but helpful: baseline mental health, personality/introversion (only if the dataset includes it)\n\n### How many observations we need (rough target + reasoning)\n- Goal: enough data so regression estimates are stable after adding controls.\n- A practical target:\n  - At least ~1,000 people for a basic model (passive + active + a handful of controls).\n  - Better: 3,000–10,000 people if we want to test subgroups (e.g., different age groups) or add more controls without the model becoming unstable.\n- If we have repeated measures (daily/weekly rows per person), then the “number of rows” can be large even if the number of people is smaller — but we still want a solid number of unique people for generalizable results.\n\n### Ideal data collection method (in a perfect world)\n- Best-quality approach is a hybrid:\n  - Phone/app logs to capture passive vs active behavior accurately (reduces memory bias), and\n  - Short surveys for stress/happiness and sleep/health.\n- If logs aren’t possible, then a well-designed survey can still work, but it’s more prone to self-report error.\n\n### How the data would be stored/organized\n- Use “tidy” structure:\n  - Each row = one person in one time window (person-week or person-day)\n  - Columns = predictors/outcomes/controls\n- Keys:\n  - Primary key could be (user_id, date/week_id) if repeated measures exist.\n- Missingness handling plan:\n  - Track missing values explicitly (don’t silently drop without checking).\n  - If only a small amount is missing, we can use listwise deletion (drop rows) but we must report how many rows we dropped.\n  - If missingness is larger, consider simple imputation (only if allowed / appropriate) and run sensitivity checks.\n\n---\n\n## 2) Real dataset sources (5 Sources)\n\nWe propose use dataset 1 for our main analysis, and treat dataset 2 to 5 as backup/validation datasets (or to strengthen the “real data sources” part of the proposal).\n\n### Dataset 1 (Main): Kaggle — Social Media User Activity Dataset\nThis is our primary dataset because it is directly designed around social media activity and includes wellbeing outcomes aligned with our research question. The dataset is expected to let us measure both passive and active social media behaviors and relate them to stress/happiness, while controlling for demographics and lifestyle variables when available. Access is straightforward for students because it is hosted on Kaggle, and we can download it manually or use the Kaggle API after setting up credentials. A key limitation is that Kaggle datasets often rely on self-reported or synthetic/compiled data, so we will clearly describe what the dataset represents once we inspect the included documentation and column names.\n\n- **URL:** https://www.kaggle.com/datasets/sadiajavedd/social-media-user-activity-dataset\n\n#### Access requirements\n- Kaggle account login\n- Accept Kaggle dataset terms\n- Optional: Kaggle API token if downloading in a notebook/script\n\n#### How it maps to our ideal dataset\n- Should contain: passive + active measures, wellbeing outcomes (stress/happiness), and at least some controls (we will confirm exact columns when we load the CSV)\n\n#### Main gaps / how we handle\n- If missing timestamps: treat it as cross-sectional (one row per person) and avoid time-based claims\n- If missing key controls: we’ll use what exists and clearly state remaining confounding risk\n\n---\n\n### Dataset 2: Understanding Society (UK Household Longitudinal Study) — social media “looking” vs “posting”\nThis dataset is useful because it includes survey measures that separate looking at social networking sites (a passive-like behavior) from posting on social networking sites (an active-like behavior). That matches our “passive vs active” concept well, and it is paired with many wellbeing and background variables commonly used in social science (e.g., mental health and demographics). Access requires registering through the UK Data Service, which is extra steps compared to Kaggle, but still realistic for a university project. A limitation is that it is UK-based and the exact wellbeing measures may differ from “stress/happiness,” so we may need to use related outcomes (e.g., psychological distress or life satisfaction) if we use it for replication.\n\n- **URL:** https://www.understandingsociety.ac.uk/\n\n#### Access requirements:\n- Registration (typically through the UK Data Service)\n- Agree to an End User License (EUL) / academic use conditions\n\n#### Important variables (examples):\n- `smlook` (frequency of looking at social networking sites)\n- `smpost` (frequency of posting on social networking sites)\n- Plus many demographic/SES variables and wellbeing/mental health measures\n\n#### Main gaps / how we handle\n- Different country + potentially different outcome measures → treat as “validation/replication,” not a direct replacement for our main dataset\n\n---\n\n### Dataset 3: CDC Youth Risk Behavior Surveillance System (YRBSS) — social media use + health/wellbeing proxies\nYRBSS is a strong backup because it is a large, public, well-documented survey and includes youth health and wellbeing-related variables (mental health indicators, sleep, physical activity, etc.). It can support analysis linking amount/frequency of social media use to wellbeing-related outcomes, especially if we want a well-known, credible dataset source. However, it typically won’t separate passive vs active usage in detail, so it is better as a “bigger picture” comparison dataset rather than a perfect match. Access is easy: CDC provides documentation and data downloads.\n\n- **URL:** https://www.cdc.gov/yrbs/index.html\n\n#### Access requirements\n- Public access (no API key needed); follow CDC data use guidance\n\n#### Important variables (typical categories)\n- Social media use frequency/time (when included in that survey year)\n- Sleep and physical activity variables (commonly included)\n- Mental health-related indicators (commonly included)\n\n#### Main gaps / how we handle\n- Teen-focused (not all ages) + limited “active vs passive” detail → use for robustness checks only, and keep conclusions narrow\n\n---\n\n### Dataset 4: Kaggle - Social Media and Mental Health\nThis dataset is helpful because it is directly about social media usage and mental health/wellbeing, and it is easy to access and analyze in a class setting. It can provide alternative measures of wellbeing (mental health indicators) and usage (time spent / habits), which can be used to check whether patterns match our main dataset’s results. Kaggle access is simple (download or API), which makes it “low friction” for the project timeline. A limitation is that the exact definitions of variables may not perfectly match our passive vs active definitions, so we would map the closest available variables and be explicit about the mismatch.\n\n- **URL:** https://www.kaggle.com/datasets/souvikahmed071/social-media-and-mental-health\n\n#### Access requirements\n- Kaggle login + terms acceptance\n- Optional Kaggle API token for programmatic download\n\n#### Main gaps / how we handle\n- If “passive vs active” is not explicit → create proxy variables (e.g., scrolling time as passive; posting/commenting frequency as active) only if the dataset supports it\n\n---\n\n### Dataset 5: GitHub project (data description) — Social Media Usage & Emotional Well-Being (columns include posts/likes/comments/messages)\nThis source is useful because it clearly lists a clean set of activity variables that match our active vs passive framing: daily usage time (passive-ish exposure) plus posts, likes, comments, and messages (active interaction). It also includes an emotion/wellbeing-related outcome field (dominant emotion), so it can be used as an additional dataset to test similar relationships even if the outcome is not exactly “stress/happiness.” This can strengthen our proposal by showing we have multiple realistic dataset options with the right activity structure. A limitation is that it may not include the exact control variables we want (SES/lifestyle), so it is best as a “supporting dataset,” not the main one.\n\n- **URL:** https://github.com/JamshedAli18/Social-Media-Usage-and-Emotional-Well-Being-Analysis\n\n#### Important variables (as listed)\n- `Daily_Usage_Time` (minutes) (passive exposure proxy)\n- `Posts_Per_Day`, `Likes_Received_Per_Day`, `Comments_Received_Per_Day`, `Messages_Sent_Per_Day` (active interaction proxies)\n\n#### Main gaps / how we handle\n- Outcome is emotion category (not stress/happiness) and controls may be limited to use only as secondary/illustrative analysis"
]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
  "## Ethics\n\nInstructions: Keep the contents of this cell. For each item on the checklist\n-  put an X there if you've considered the item\n-  IF THE ITEM IS RELEVANT place a short paragraph after the checklist item discussing the issue.\n  \nItems on this checklist are meant to provoke discussion among good-faith actors who take their ethical responsibilities seriously. Your teams will document these discussions and decisions for posterity using this section.  You don't have to solve these problems, you just have to acknowledge any potential harm no matter how unlikely.\n\nHere is a [list of real world examples](https://deon.drivendata.org/examples/) for each item in the checklist that can refer to.\n\n[![Deon badge](https://img.shields.io/badge/ethics%20checklist-deon-brightgreen.svg?style=popout-square)](http://deon.drivendata.org/)\n\n### A. Data Collection\n - [X] **A.1 Informed consent**: If there are human subjects, have they given informed consent, where subjects affirmatively opt-in and have a clear understanding of the data uses to which they consent?\n\n> This project uses secondary data from publicly available sources such as Kaggle and PubMed. While we do not collect data directly from participants, there is a risk that individuals did not anticipate all future uses of their data. We mitigate this by using the data only for aggregate research purposes and avoiding sensitive individual-level conclusions.\n\n - [X] **A.2 Collection bias**: Have we considered sources of bias that could be introduced during data collection and survey design and taken steps to mitigate those?\n\n> Social media wellbeing datasets often overrepresent younger, more active users or specific regions, which may bias results. If ignored, this could lead to misleading conclusions about broader populations. We will report these limitations and avoid generalizing beyond the sampled groups.\n\n - [X] **A.3 Limit PII exposure**: Have we considered ways to minimize exposure of personally identifiable information (PII) for example through anonymization or not collecting information that isn't relevant for analysis?\n\n> The datasets we use are de-identified, but indirect identification is still possible from detailed demographics or behavior. To reduce risk, we will drop unnecessary fields, avoid small subgroup reporting, and present only aggregated results.\n\n - [X] **A.4 Downstream bias mitigation**: Have we considered ways to enable testing downstream results for biased outcomes (e.g., collecting data on protected group status like race or gender)?\n\n> If demographic variables are available, we will use them to test whether associations differ across groups (ex: gender, age ranges) and report any differences. We will be careful not to interpret group differences as biological or moral claims, and we will highlight dataset limitations.\n\n### B. Data Storage\n - [X] **B.1 Data security**: Do we have a plan to protect and secure data (e.g., encryption at rest and in transit, access controls on internal users and third parties, access logs, and up-to-date software)?\n\n> Data will be stored in secure, course-approved environments with access limited to project members. Although the data are public and anonymized, restricting access reduces unnecessary exposure.\n\n - [] **B.2 Right to be forgotten**: Do we have a mechanism through which an individual can request their personal information be removed?\n\n> Not directly applicable because we are using public, de-identified secondary datasets and do not control data collection. If any dataset includes removals or takedown procedures, we will follow the dataset’s terms and avoid downloading unnecessary copies.\n\n - [X] **B.3 Data retention plan**: Is there a schedule or plan to delete the data after it is no longer needed?\n\n> We will keep datasets only for the course project duration and delete local copies afterward. Keeping fewer copies reduces exposure risk and supports responsible data handling.\n\n### C. Analysis\n - [X] **C.1 Missing perspectives**: Have we sought to address blindspots in the analysis through engagement with relevant stakeholders (e.g., checking assumptions and discussing implications with affected communities and subject matter experts)?\n\n> Wellbeing and social media experience vary by culture, age, and context, and the dataset may not capture these differences. We will interpret results cautiously and avoid universal claims, using prior literature to contextualize findings.\n\n - [X] **C.2 Dataset bias**: Have we examined the data for possible sources of bias and taken steps to mitigate or address these biases (e.g., stereotype perpetuation, confirmation bias, imbalanced classes, or omitted confounding variables)?\n\n> We will look for bias in the dataset, like missing data, uneven group sizes, or important factors that aren’t included. We will report these issues and control for key variables when possible, instead of acting like the results are causal.\n\n - [X] **C.3 Honest representation**: Are our visualizations, summary statistics, and reports designed to honestly represent the underlying data?\n\n> We will avoid misleading visuals and cherry-picking. We will report effect sizes with uncertainty and include null results when they occur.\n\n - [X] **C.4 Privacy in analysis**: Have we ensured that data with PII are not used or displayed unless necessary for the analysis?\n\n> We will not display or publish any PPI data. Results will be reported only in aggregated form, and we will avoid subgroup breakdowns that could enable re-identification.\n\n - [X] **C.5 Auditability**: Is the process of generating the analysis well documented and reproducible if we discover issues in the future?\n\n> We will document data cleaning and analysis steps and keep code organized so results can be reproduced. This supports accountability if errors or ethical concerns are found later.\n\n### D. Modeling\n - [X] **D.1 Proxy discrimination**: Have we ensured that the model does not rely on variables or proxies for variables that are unfairly discriminatory?\n\n> We will avoid using variables that can “stand in” for protected traits (like race or income) in a way that could be unfair. If we include those variables as controls, we will explain why and be careful how we interpret them.\n\n - [X] **D.2 Fairness across groups**: Have we tested model results for fairness with respect to different affected groups (e.g., tested for disparate error rates)?\n\n> We will check whether results look different across demographic groups (when that data is available). If we build a prediction model, we will also compare error rates across groups and report any big gaps.\n\n - [X] **D.3 Metric selection**: Have we considered the effects of optimizing for our defined metrics and considered additional metrics?\n\n> We will choose evaluation metrics that match the task and report multiple metrics where relevant. We will avoid optimizing a single metric that could hide subgroup harms.\n\n - [X] **D.4 Explainability**: Can we explain in understandable terms a decision the model made in cases where a justification is needed?\n\n> We will use simple models like regression so it’s clear why the model gives a result. We will explain the main factors and what they mean in plain language.\n\n - [X] **D.5 Communicate limitations**: Have we communicated the shortcomings, limitations, and biases of the model to relevant stakeholders in ways that can be generally understood?\n\n> We will explain our findings in plain language and note what the model cannot conclude. We will highlight bias and missing factors and keep conclusions within what the data supports.\n\n### E. Deployment\n - [] **E.1 Monitoring and evaluation**: Do we have a clear plan to monitor the model and its impacts after it is deployed (e.g., performance monitoring, regular audit of sample predictions, human review of high-stakes decisions, reviewing downstream impacts of errors or low-confidence decisions, testing for concept drift)?\n\n> Not applicable because we are not deploying a model or system. This is a course project analysis only.\n\n - [] **E.2 Redress**: Have we discussed with our organization a plan for response if users are harmed by the results (e.g., how does the data science team evaluate these cases and update analysis and models to prevent future harm)?\n\n> Not applicable because our results will not be used to make decisions about individuals. If we find something misleading or harmful, we will revise the analysis and how we report it.\n\n - [X] **E.3 Roll back**: Is there a way to turn off or roll back the model in production if necessary?\n \n> Not applicable because there is no production deployment. If issues are found, we can update or remove the analysis/report.\n\n - [X] **E.4 Unintended use**: Have we taken steps to identify and prevent unintended uses and abuse of the model and do we have a plan to monitor these once the model is deployed?\n \n> People could misuse our results to say social media is always good or always bad, or to shame people with mental health issues. We will reduce this risk by saying our findings are correlational, clearly listing limits, and keeping conclusions only within what the data supports."
]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Team Expectations "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Instructions: REPLACE the contents of this cell with your work\n",
    "  \n",
    "Read over the [COGS108 Team Policies](https://github.com/COGS108/Projects/blob/master/COGS108_TeamPolicies.md) individually. Then, include your group’s expectations of one another for successful completion of your COGS108 project below. Discuss and agree on what all of your expectations are. Discuss how your team will communicate throughout the quarter and consider how you will communicate respectfully should conflicts arise. By including each member’s name above and by adding their name to the submission, you are indicating that you have read the COGS108 Team Policies, accept your team’s expectations below, and have every intention to fulfill them. These expectations are for your team’s use and benefit — they won’t be graded for their details.\n",
    "\n",
    "* *Team Expectation 1*\n",
    "* *Team Expectation 2*\n",
    "* *Team Expecation 3*\n",
    "* ..."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Project Timeline Proposal"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Instructions: REPLACE the contents of this cell with your work\n",
    "\n",
    "Specify your team's specific project timeline. An example timeline has been provided. Changes the dates, times, names, and details to fit your group's plan.\n",
    "\n",
    "If you think you will need any special resources or training outside what we have covered in COGS 108 to solve your problem, then your proposal should state these clearly. For example, if you have selected a problem that involves implementing multiple neural networks, please state this so we can make sure you know what you’re doing and so we can point you to resources you will need to implement your project. Note that you are not required to use outside methods.\n",
    "\n",
    "\n",
    "\n",
    "| Meeting Date  | Meeting Time| Completed Before Meeting  | Discuss at Meeting |\n",
    "|---|---|---|---|\n",
    "| 1/20  |  1 PM | Read & Think about COGS 108 expectations; brainstorm topics/questions  | Determine best form of communication; Discuss and decide on final project topic; discuss hypothesis; begin background research | \n",
    "| 1/26  |  10 AM |  Do background research on topic | Discuss ideal dataset(s) and ethics; draft project proposal | \n",
    "| 2/1  | 10 AM  | Edit, finalize, and submit proposal; Search for datasets  | Discuss Wrangling and possible analytical approaches; Assign group members to lead each specific part   |\n",
    "| 2/14  | 6 PM  | Import & Wrangle Data (Ant Man); EDA (Hulk) | Review/Edit wrangling/EDA; Discuss Analysis Plan   |\n",
    "| 2/23  | 12 PM  | Finalize wrangling/EDA; Begin Analysis (Iron Man; Thor) | Discuss/edit Analysis; Complete project check-in |\n",
    "| 3/13  | 12 PM  | Complete analysis; Draft results/conclusion/discussion (Wasp)| Discuss/edit full project |\n",
    "| 3/20  | Before 11:59 PM  | NA | Turn in Final Project & Group Project Surveys |"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  },
  "vscode": {
   "interpreter": {
    "hash": "16b860a9f5fc21240e9d88c0ee13691518c3ce67be252e54a03b9b5b11bd3c7a"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
